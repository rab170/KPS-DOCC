{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# using two datasets, they are 20newsgroups and Reuters-21578\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import random\n",
    "import json\n",
    "import collections\n",
    "from itertools import compress\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.datasets import reuters as reuters2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Loading Datasets and Classify Documents with Baselines\n",
    "\n",
    "Two baseline system is used: one is a multilayer perceptron and the other is support vector machine.\n",
    "Both of them classify documents using tf-idf vectors of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    docs_train = []\n",
    "    docs_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    target_names = []\n",
    "    \n",
    "    # using reuters dataset from keras package\n",
    "    if dataset_name == 'reuters':\n",
    "#         target_names = reuters.categories()\n",
    "#         for doc_id in reuters.fileids():\n",
    "#             file_target_list = reuters.categories(doc_id)\n",
    "#             if doc_id.startswith(\"train\"):\n",
    "#                 docs_train.append(reuters.raw(doc_id))\n",
    "#                 y = []\n",
    "#                 for file_target in file_target_list:                    \n",
    "#                     y.append(target_names.index(file_target))\n",
    "#                 y_train.append(y)\n",
    "#             else:\n",
    "#                 docs_test.append(reuters.raw(doc_id))\n",
    "#                 y = []\n",
    "#                 for file_target in file_target_list:\n",
    "#                     y.append(target_names.index(file_target))\n",
    "#                 y_test.append(y)\n",
    "        word_index = reuters2.get_word_index(path=\"reuters_word_index.json\")\n",
    "        inverse_word_dict = np.ndarray(shape=(len(word_index)+1,), dtype=object)\n",
    "        for key in word_index:\n",
    "            index = word_index[key]\n",
    "            inverse_word_dict[index] = key\n",
    "\n",
    "        print('Loading reuters dataset...')\n",
    "        (x_train, y_train), (x_test, y_test) = reuters2.load_data(test_split=0.2)\n",
    "        \n",
    "        for x in x_train:\n",
    "            x = [t for t in x if t < len(word_index)]\n",
    "            docs_train.append(' '.join(inverse_word_dict[x]))\n",
    "        print(len(docs_train), 'train docs', len(y_train))\n",
    "        \n",
    "        for x in x_test:\n",
    "            x = [t for t in x if t < len(word_index)]\n",
    "            docs_test.append(' '.join(inverse_word_dict[x]))\n",
    "        print(len(docs_test), 'test docs', len(y_test))\n",
    "        \n",
    "        target_names = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                        '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                        '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                        '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                        '1', '2', '3', '4', '5', '6']\n",
    "    elif dataset_name == '20newsgroups':\n",
    "        newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "        docs_train = newsgroups_train.data\n",
    "        y_train = newsgroups_train.target\n",
    "        newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "        docs_test = newsgroups_test.data\n",
    "        y_test = newsgroups_test.target\n",
    "        target_names = newsgroups_train.target_names\n",
    "    \n",
    "    print(len(docs_train), \"training documents are loaded.\")\n",
    "    print(len(docs_test), \"test documents are loaded.\\n\")\n",
    "    \n",
    "    return docs_train, y_train, docs_test, y_test, np.array(target_names)\n",
    "\n",
    "# convert documents to bag of word vectors\n",
    "def doc_2_matrix(vocab_size, docs_train, y_train, docs_test, y_test):\n",
    "    \n",
    "    MAX_NB_WORDS = vocab_size\n",
    "    MAX_SEQUENCE_LENGTH = 2000\n",
    "    \n",
    "    # fit the tokenizer with the corpus\n",
    "    tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "    docs = []\n",
    "    docs.extend(docs_train)\n",
    "    docs.extend(docs_test)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    # vectorize texts into 2D integer tensors\n",
    "    # mode: \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\")\n",
    "    x_train_m = tokenizer.texts_to_matrix(docs_train, mode='tfidf')\n",
    "    y_train_m = to_categorical(np.asarray(y_train))\n",
    "    print('Shape of x_train_m:', x_train_m.shape)\n",
    "    print('Shape of y_train_m:', y_train_m.shape)\n",
    "    \n",
    "    x_test_m = tokenizer.texts_to_matrix(docs_test, mode='tfidf')\n",
    "    y_test_m = to_categorical(np.asarray(y_test))\n",
    "    print('Shape of x_test_m:', x_test_m.shape)\n",
    "    print('Shape of y_test_m:', y_test_m.shape)\n",
    "    \n",
    "    return x_train_m, y_train_m, x_test_m, y_test_m\n",
    "\n",
    "# convert documents to word index sequences\n",
    "def doc_2_sequences(docs_train, y_train, docs_test, y_test):\n",
    "    # vectorize the text samples into a 2D integer tensor\n",
    "    MAX_NB_WORDS = 60000\n",
    "    MAX_SEQUENCE_LENGTH = 2000\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "    docs = []\n",
    "    docs.extend(docs_train)\n",
    "    docs.extend(docs_test)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    x_train_m = pad_sequences(tokenizer.texts_to_sequences(docs_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    y_train_m = to_categorical(np.asarray(y_train))\n",
    "    print('Shape of x_train_m:', x_train_m.shape)\n",
    "    print('Shape of y_train_m:', y_train_m.shape)\n",
    "    \n",
    "    x_test_m = pad_sequences(tokenizer.texts_to_sequences(docs_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    y_test_m = to_categorical(np.asarray(y_test))\n",
    "    print('Shape of x_test_m:', x_test_m.shape)\n",
    "    print('Shape of y_test_m:', y_test_m.shape)\n",
    "    \n",
    "    return x_train_m, y_train_m, x_test_m, y_test_m\n",
    "\n",
    "def mlp_base_line(vocab_size, num_classes, x_train_m, y_train_m, x_test_m, y_test_m, epochs, dataset, isload):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    if isload == False:\n",
    "        print('Building a MLP baseline model...')\n",
    "#         model = Sequential()\n",
    "        model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    else:\n",
    "        print('loading a pretrained MLP baseline model...')\n",
    "        model = load_model('Model/' + dataset + '_cnn_300.h5')\n",
    "        \n",
    "    batch_size = 128\n",
    "    history = model.fit(x_train_m, y_train_m,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        verbose = 1,\n",
    "                        validation_split = 0.1)\n",
    "    model.save('Model/' + dataset + '_cnn_300.h5')\n",
    "\n",
    "    result = model.evaluate(x_test_m, y_test_m, batch_size = batch_size, verbose = 1)\n",
    "    print('\\nTest score:', score[0], 'Test accuracy:', score[1])\n",
    "    \n",
    "def svm_test(X, y, C1, X_test, y_test, C2):\n",
    "    for c_value in C1:\n",
    "        clf = svm.SVC(C=c_value, gamma=1/len(X[0])) \n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        counter = 0;\n",
    "        for idx, pred in enumerate(y_pred):\n",
    "            if y_test[idx] != pred:\n",
    "                counter = counter + 1\n",
    "        print(\"RBF: Correct rate =\", 1 - counter/len(y_test), \" When C =\", c_value)\n",
    "        \n",
    "    for c_value in C2:\n",
    "        lin_clf = svm.LinearSVC(C=c_value)\n",
    "        lin_clf.fit(X, y)\n",
    "        y_pred = lin_clf.predict(X_test)\n",
    "\n",
    "        counter = 0;\n",
    "        for idx, pred in enumerate(y_pred):\n",
    "            if y_test[idx] != pred:\n",
    "                counter = counter + 1\n",
    "        print(\"Linear: Correct rate =\", 1 - counter/len(y_test), \" When C =\", c_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 testing documents are loaded.\n",
      "\n",
      "11314 training documents are loaded.\n",
      "7532 testing documents are loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[reuters_docs_train, reuters_docs_test, reuters_y_train, reuters_y_test,\n",
    " reuters_target_names] = load_dataset('reuters')\n",
    "[news_docs_train, news_docs_test, news_y_train, news_y_test,\n",
    " news_target_names] = load_dataset('20newsgroups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using MLP baseline to classifiy documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 testing documents are loaded.\n",
      "\n",
      "Found 30976 unique tokens.\n",
      "Shape of x_train_m: (8982, 30000)\n",
      "Shape of y_train_m: (8982, 46)\n",
      "Shape of x_test_m: (2246, 30000)\n",
      "Shape of y_test_m: (2246, 46)\n",
      "46 targets\n",
      "\n",
      "loading a pretrained MLP baseline model...\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1494 - acc: 0.9703 - val_loss: 1.5429 - val_acc: 0.7786\n",
      "Epoch 2/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1339 - acc: 0.9707 - val_loss: 1.6103 - val_acc: 0.7731\n",
      "Epoch 3/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1270 - acc: 0.9735 - val_loss: 1.6163 - val_acc: 0.7675\n",
      "Epoch 4/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1296 - acc: 0.9743 - val_loss: 1.6539 - val_acc: 0.7709\n",
      "Epoch 5/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1288 - acc: 0.9730 - val_loss: 1.7010 - val_acc: 0.7675\n",
      "Epoch 6/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1317 - acc: 0.9739 - val_loss: 1.6835 - val_acc: 0.7753\n",
      "Epoch 7/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1328 - acc: 0.9740 - val_loss: 1.6851 - val_acc: 0.7597\n",
      "Epoch 8/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1308 - acc: 0.9724 - val_loss: 1.6632 - val_acc: 0.7697\n",
      "Epoch 9/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1241 - acc: 0.9738 - val_loss: 1.6396 - val_acc: 0.7731\n",
      "Epoch 10/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1515 - acc: 0.9706 - val_loss: 1.6762 - val_acc: 0.7608\n",
      "Epoch 11/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1361 - acc: 0.9743 - val_loss: 1.6894 - val_acc: 0.7675\n",
      "Epoch 12/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1544 - acc: 0.9720 - val_loss: 1.7547 - val_acc: 0.7620\n",
      "Epoch 13/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1524 - acc: 0.9713 - val_loss: 1.7948 - val_acc: 0.7453\n",
      "Epoch 14/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1670 - acc: 0.9707 - val_loss: 1.7791 - val_acc: 0.7519\n",
      "Epoch 15/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1547 - acc: 0.9713 - val_loss: 1.7519 - val_acc: 0.7597\n",
      "Epoch 16/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1593 - acc: 0.9727 - val_loss: 1.7531 - val_acc: 0.7620\n",
      "Epoch 17/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1567 - acc: 0.9717 - val_loss: 1.8135 - val_acc: 0.7464\n",
      "Epoch 18/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1658 - acc: 0.9702 - val_loss: 1.8213 - val_acc: 0.7586\n",
      "Epoch 19/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1492 - acc: 0.9735 - val_loss: 1.8087 - val_acc: 0.7475\n",
      "Epoch 20/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1624 - acc: 0.9719 - val_loss: 1.7304 - val_acc: 0.7519\n",
      "Epoch 21/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1640 - acc: 0.9702 - val_loss: 1.7771 - val_acc: 0.7531\n",
      "Epoch 22/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1798 - acc: 0.9697 - val_loss: 1.7277 - val_acc: 0.7675\n",
      "Epoch 23/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1726 - acc: 0.9702 - val_loss: 1.7024 - val_acc: 0.7575\n",
      "Epoch 24/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1729 - acc: 0.9718 - val_loss: 1.7343 - val_acc: 0.7608\n",
      "Epoch 25/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1640 - acc: 0.9708 - val_loss: 1.7357 - val_acc: 0.7642\n",
      "Epoch 26/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1667 - acc: 0.9680 - val_loss: 1.8381 - val_acc: 0.7586\n",
      "Epoch 27/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1812 - acc: 0.9699 - val_loss: 1.8319 - val_acc: 0.7531\n",
      "Epoch 28/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1608 - acc: 0.9703 - val_loss: 1.7827 - val_acc: 0.7631\n",
      "Epoch 29/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1798 - acc: 0.9692 - val_loss: 1.8211 - val_acc: 0.7553\n",
      "Epoch 30/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1617 - acc: 0.9707 - val_loss: 1.9087 - val_acc: 0.7475\n",
      "Epoch 31/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2180 - acc: 0.9641 - val_loss: 1.8599 - val_acc: 0.7430\n",
      "Epoch 32/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1996 - acc: 0.9639 - val_loss: 1.8246 - val_acc: 0.7453\n",
      "Epoch 33/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2249 - acc: 0.9647 - val_loss: 1.8104 - val_acc: 0.7542\n",
      "Epoch 34/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2079 - acc: 0.9652 - val_loss: 1.8506 - val_acc: 0.7553\n",
      "Epoch 35/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2124 - acc: 0.9631 - val_loss: 1.8322 - val_acc: 0.7453\n",
      "Epoch 36/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2082 - acc: 0.9663 - val_loss: 1.8477 - val_acc: 0.7453\n",
      "Epoch 37/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.2137 - acc: 0.9642 - val_loss: 1.8152 - val_acc: 0.7497\n",
      "Epoch 38/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2046 - acc: 0.9661 - val_loss: 1.7829 - val_acc: 0.7608\n",
      "Epoch 39/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2006 - acc: 0.9645 - val_loss: 1.7461 - val_acc: 0.7620\n",
      "Epoch 40/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2012 - acc: 0.9667 - val_loss: 1.8404 - val_acc: 0.7597\n",
      "Epoch 41/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2046 - acc: 0.9660 - val_loss: 1.8206 - val_acc: 0.7497\n",
      "Epoch 42/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2156 - acc: 0.9645 - val_loss: 1.8131 - val_acc: 0.7475\n",
      "Epoch 43/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2219 - acc: 0.9633 - val_loss: 1.8234 - val_acc: 0.7486\n",
      "Epoch 44/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2111 - acc: 0.9644 - val_loss: 1.8723 - val_acc: 0.7475\n",
      "Epoch 45/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2176 - acc: 0.9641 - val_loss: 1.8315 - val_acc: 0.7620\n",
      "Epoch 46/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2017 - acc: 0.9649 - val_loss: 1.8492 - val_acc: 0.7508\n",
      "Epoch 47/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2074 - acc: 0.9640 - val_loss: 1.8672 - val_acc: 0.7575\n",
      "Epoch 48/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2104 - acc: 0.9626 - val_loss: 1.8500 - val_acc: 0.7575\n",
      "Epoch 49/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1957 - acc: 0.9668 - val_loss: 1.8705 - val_acc: 0.7508\n",
      "Epoch 50/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2094 - acc: 0.9650 - val_loss: 1.8786 - val_acc: 0.7553\n",
      "Epoch 51/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.2154 - acc: 0.9640 - val_loss: 1.8825 - val_acc: 0.7464\n",
      "Epoch 52/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1994 - acc: 0.9642 - val_loss: 1.8723 - val_acc: 0.7564\n",
      "Epoch 53/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1844 - acc: 0.9668 - val_loss: 1.8530 - val_acc: 0.7631\n",
      "Epoch 54/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1884 - acc: 0.9650 - val_loss: 1.8367 - val_acc: 0.7608\n",
      "Epoch 55/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1888 - acc: 0.9660 - val_loss: 1.8288 - val_acc: 0.7575\n",
      "Epoch 56/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1981 - acc: 0.9645 - val_loss: 1.8346 - val_acc: 0.7597\n",
      "Epoch 57/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1930 - acc: 0.9650 - val_loss: 1.8245 - val_acc: 0.7586\n",
      "Epoch 58/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1890 - acc: 0.9647 - val_loss: 1.8504 - val_acc: 0.7519\n",
      "Epoch 59/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1758 - acc: 0.9668 - val_loss: 1.8741 - val_acc: 0.7341\n",
      "Epoch 60/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1880 - acc: 0.9662 - val_loss: 1.8943 - val_acc: 0.7508\n",
      "Epoch 61/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1840 - acc: 0.9655 - val_loss: 1.8816 - val_acc: 0.7442\n",
      "Epoch 62/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1839 - acc: 0.9665 - val_loss: 1.8880 - val_acc: 0.7475\n",
      "Epoch 63/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1894 - acc: 0.9634 - val_loss: 1.8652 - val_acc: 0.7508\n",
      "Epoch 64/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1866 - acc: 0.9656 - val_loss: 1.8932 - val_acc: 0.7475\n",
      "Epoch 65/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1792 - acc: 0.9645 - val_loss: 1.9499 - val_acc: 0.7519\n",
      "Epoch 66/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1777 - acc: 0.9668 - val_loss: 1.9634 - val_acc: 0.7508\n",
      "Epoch 67/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1873 - acc: 0.9647 - val_loss: 1.9608 - val_acc: 0.7419\n",
      "Epoch 68/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1847 - acc: 0.9638 - val_loss: 1.9863 - val_acc: 0.7519\n",
      "Epoch 69/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1925 - acc: 0.9656 - val_loss: 1.9720 - val_acc: 0.7475\n",
      "Epoch 70/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1751 - acc: 0.9666 - val_loss: 2.0092 - val_acc: 0.7375\n",
      "Epoch 71/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1862 - acc: 0.9656 - val_loss: 1.9824 - val_acc: 0.7464\n",
      "Epoch 72/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1776 - acc: 0.9671 - val_loss: 1.9792 - val_acc: 0.7475\n",
      "Epoch 73/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1873 - acc: 0.9644 - val_loss: 2.0073 - val_acc: 0.7519\n",
      "Epoch 74/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1812 - acc: 0.9652 - val_loss: 1.9866 - val_acc: 0.7464\n",
      "Epoch 75/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1646 - acc: 0.9678 - val_loss: 2.0133 - val_acc: 0.7442\n",
      "Epoch 76/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1770 - acc: 0.9640 - val_loss: 2.0254 - val_acc: 0.7475\n",
      "Epoch 77/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1869 - acc: 0.9647 - val_loss: 2.0551 - val_acc: 0.7508\n",
      "Epoch 78/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1791 - acc: 0.9676 - val_loss: 2.0043 - val_acc: 0.7419\n",
      "Epoch 79/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1720 - acc: 0.9646 - val_loss: 2.0318 - val_acc: 0.7419\n",
      "Epoch 80/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1807 - acc: 0.9665 - val_loss: 2.0167 - val_acc: 0.7453\n",
      "Epoch 81/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1755 - acc: 0.9633 - val_loss: 2.0323 - val_acc: 0.7475\n",
      "Epoch 82/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1719 - acc: 0.9663 - val_loss: 2.0170 - val_acc: 0.7419\n",
      "Epoch 83/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1567 - acc: 0.9671 - val_loss: 2.0107 - val_acc: 0.7419\n",
      "Epoch 84/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1722 - acc: 0.9649 - val_loss: 2.0403 - val_acc: 0.7386\n",
      "Epoch 85/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1683 - acc: 0.9651 - val_loss: 2.0109 - val_acc: 0.7408\n",
      "Epoch 86/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1729 - acc: 0.9655 - val_loss: 2.0381 - val_acc: 0.7453\n",
      "Epoch 87/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1642 - acc: 0.9656 - val_loss: 2.0621 - val_acc: 0.7375\n",
      "Epoch 88/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1640 - acc: 0.9663 - val_loss: 2.1127 - val_acc: 0.7308\n",
      "Epoch 89/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1775 - acc: 0.9659 - val_loss: 2.0473 - val_acc: 0.7453\n",
      "Epoch 90/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1699 - acc: 0.9654 - val_loss: 2.0898 - val_acc: 0.7386\n",
      "Epoch 91/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1689 - acc: 0.9667 - val_loss: 2.0937 - val_acc: 0.7341\n",
      "Epoch 92/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1722 - acc: 0.9640 - val_loss: 2.1066 - val_acc: 0.7297\n",
      "Epoch 93/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1673 - acc: 0.9668 - val_loss: 2.1098 - val_acc: 0.7375\n",
      "Epoch 94/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1730 - acc: 0.9649 - val_loss: 2.1044 - val_acc: 0.7453\n",
      "Epoch 95/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1570 - acc: 0.9647 - val_loss: 2.1070 - val_acc: 0.7464\n",
      "Epoch 96/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1690 - acc: 0.9661 - val_loss: 2.1301 - val_acc: 0.7408\n",
      "Epoch 97/100\n",
      "8083/8083 [==============================] - 18s - loss: 0.1625 - acc: 0.9666 - val_loss: 2.1371 - val_acc: 0.7408\n",
      "Epoch 98/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1675 - acc: 0.9641 - val_loss: 2.1233 - val_acc: 0.7386\n",
      "Epoch 99/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1597 - acc: 0.9666 - val_loss: 2.1539 - val_acc: 0.7375\n",
      "Epoch 100/100\n",
      "8083/8083 [==============================] - 17s - loss: 0.1694 - acc: 0.9667 - val_loss: 2.1494 - val_acc: 0.7364\n",
      "2246/2246 [==============================] - 1s     \n",
      "\n",
      "Test score: 1.29018464284 Test accuracy: 0.771593946171\n"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('reuters') # 20newsgroups\n",
    "vocab_size = 30000\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_matrix(vocab_size, docs_train, y_train, docs_test, y_test)\n",
    "print(len(target_names), 'targets\\n')\n",
    "mlp_base_line(x_train_m.shape[1], len(target_names), x_train_m, y_train_m, x_test_m, y_test_m, 100, 'reuters', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SVM to classify documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 test documents are loaded.\n",
      "\n",
      "Found 30976 unique tokens.\n",
      "Shape of x_train_m: (8982, 30000)\n",
      "Shape of y_train_m: (8982, 46)\n",
      "Shape of x_test_m: (2246, 30000)\n",
      "Shape of y_test_m: (2246, 46)\n",
      "46 targets\n",
      "\n",
      "Linear:Correct rate = 0.8063223508459484 When C = 5e-05\n",
      "Linear:Correct rate = 0.8227960819234195 When C = 0.0001\n",
      "Linear:Correct rate = 0.8268032056990204 When C = 0.0005\n",
      "Linear:Correct rate = 0.825912733748887 When C = 0.001\n",
      "Linear:Correct rate = 0.8147818343722173 When C = 0.01\n",
      "Linear:Correct rate = 0.807212822796082 When C = 0.05\n",
      "Linear:Correct rate = 0.7960819234194123 When C = 0.5\n",
      "Linear:Correct rate = 0.7853962600178095 When C = 1.0\n"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('reuters') # 20newsgroups reuters\n",
    "vocab_size = 30000\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_matrix(vocab_size, docs_train, y_train, docs_test, y_test)\n",
    "C1 = [] # RBF [8, 16, 32, 64, 128, 256]\n",
    "C2 = [0.00005, 0.0001, 0.0005, 0.001, 0.01, 0.05, 0.5, 1.0] # Linear\n",
    "print(len(target_names), 'targets\\n')\n",
    "svm_test(x_train_m, y_train, C1, x_test_m, y_test, C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 training documents are loaded.\n",
      "7532 test documents are loaded.\n",
      "\n",
      "Found 179209 unique tokens.\n",
      "Shape of x_train_m: (11314, 30000)\n",
      "Shape of y_train_m: (11314, 20)\n",
      "Shape of x_test_m: (7532, 30000)\n",
      "Shape of y_test_m: (7532, 20)\n",
      "20 targets\n",
      "\n",
      "Linear:Correct rate = 0.8572756240042485 When C = 5e-05\n",
      "Linear:Correct rate = 0.8596654275092936 When C = 0.0001\n",
      "Linear:Correct rate = 0.8570100902814657 When C = 0.0005\n",
      "Linear:Correct rate = 0.8511683483802444 When C = 0.001\n",
      "Linear:Correct rate = 0.8370950610727562 When C = 0.01\n",
      "Linear:Correct rate = 0.8304567180031864 When C = 0.05\n",
      "Linear:Correct rate = 0.8171800318640468 When C = 0.5\n",
      "Linear:Correct rate = 0.8097450876261285 When C = 1.0\n"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('20newsgroups') # 20newsgroups reuters\n",
    "vocab_size = 30000\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_matrix(vocab_size, docs_train, y_train, docs_test, y_test)\n",
    "C1 = [] # RBF [8, 16, 32, 64, 128, 256]\n",
    "C2 = [0.00005, 0.0001, 0.0005, 0.001, 0.01, 0.05, 0.5, 1.0] # Linear\n",
    "print(len(target_names), 'targets\\n')\n",
    "svm_test(x_train_m, y_train, C1, x_test_m, y_test, C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Convolutional Neural Network With Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Glove word embedding\n",
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'Dataset')\n",
    "vocab_size = 60000\n",
    "MAX_SEQUENCE_LENGTH = 2000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print('Loading the Glove word embedding')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.840B.300d.txt'), \"rb\") # glove.840B.300d   glove.6B.100d\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('UTF-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 test documents are loaded.\n",
      "\n",
      "Found 30976 unique tokens.\n",
      "Preparing embedding matrix\n",
      "Building CNN model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2000, 300)         9000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1996, 128)         192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 399, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 399, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 395, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 79, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 79, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 9,362,158\n",
      "Trainable params: 362,158\n",
      "Non-trainable params: 9,000,000\n",
      "_________________________________________________________________\n",
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 test documents are loaded.\n",
      "\n",
      "Found 30976 unique tokens.\n",
      "Preparing embedding matrix\n",
      "Building CNN model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 2000, 300)         9000000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1996, 128)         192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 399, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 399, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 395, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 79, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 79, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 75, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 9,362,158\n",
      "Trainable params: 362,158\n",
      "Non-trainable params: 9,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_CNN_model(dataset): # 20newsgroups reuters\n",
    "    [docs_train, y_train, docs_test, y_test, target_names] = load_dataset('reuters') # 20newsgroups reuters\n",
    "    MAX_NB_WORDS = 60000\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "    docs = []\n",
    "    docs.extend(docs_train)\n",
    "    docs.extend(docs_test)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    print('Preparing embedding matrix')\n",
    "    num_words = min(vocab_size, len(word_index))\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    # set trainable = False to keep the embeddings fixed\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    print('Building CNN model')\n",
    "    drop_rate = 0.6\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(5))\n",
    "    # rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(5))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    #model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(len(target_names), activation='softmax'))\n",
    "    # Choose the optimizer\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop', # sgd rmsprop\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "    \n",
    "model_20newsgroups = build_CNN_model('20newsgroups') # 20newsgroups reuters\n",
    "model_20newsgroups.save('Model/news_cnn_300_1.h5')\n",
    "model_reuters = build_CNN_model('reuters') # 20newsgroups reuters\n",
    "model_reuters.save('Model/reuter_cnn_300_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 CNN Model Trained With 20 Newsgroup Dataset and 300 Dim Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 training documents are loaded.\n",
      "7532 testing documents are loaded.\n",
      "\n",
      "Found 179209 unique tokens.\n",
      "Shape of x_train_m: (11314, 2000)\n",
      "Shape of y_train_m: (11314, 20)\n",
      "Shape of x_test_m: (7532, 2000)\n",
      "Shape of y_test_m: (7532, 20)\n",
      "20 targets\n",
      "\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/60\n",
      "10182/10182 [==============================] - 144s - loss: 0.0787 - acc: 0.9802 - val_loss: 0.3365 - val_acc: 0.9249\n",
      "Epoch 2/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0767 - acc: 0.9793 - val_loss: 0.3346 - val_acc: 0.9240\n",
      "Epoch 3/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0811 - acc: 0.9787 - val_loss: 0.3137 - val_acc: 0.9258\n",
      "Epoch 4/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0813 - acc: 0.9784 - val_loss: 0.2854 - val_acc: 0.9346\n",
      "Epoch 5/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0829 - acc: 0.9777 - val_loss: 0.2758 - val_acc: 0.9249\n",
      "Epoch 6/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0740 - acc: 0.9804 - val_loss: 0.2910 - val_acc: 0.9276\n",
      "Epoch 7/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0765 - acc: 0.9804 - val_loss: 0.2843 - val_acc: 0.9373\n",
      "Epoch 8/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0871 - acc: 0.9784 - val_loss: 0.2779 - val_acc: 0.9355\n",
      "Epoch 9/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0818 - acc: 0.9795 - val_loss: 0.2766 - val_acc: 0.9276\n",
      "Epoch 10/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0875 - acc: 0.9792 - val_loss: 0.3054 - val_acc: 0.9214\n",
      "Epoch 11/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0874 - acc: 0.9778 - val_loss: 0.2917 - val_acc: 0.9293\n",
      "Epoch 12/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0885 - acc: 0.9756 - val_loss: 0.2851 - val_acc: 0.9293\n",
      "Epoch 13/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0814 - acc: 0.9792 - val_loss: 0.3107 - val_acc: 0.9240\n",
      "Epoch 14/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0683 - acc: 0.9805 - val_loss: 0.2928 - val_acc: 0.9267\n",
      "Epoch 15/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0731 - acc: 0.9802 - val_loss: 0.3071 - val_acc: 0.9267\n",
      "Epoch 16/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0801 - acc: 0.9804 - val_loss: 0.3083 - val_acc: 0.9214\n",
      "Epoch 17/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0723 - acc: 0.9807 - val_loss: 0.3260 - val_acc: 0.9223\n",
      "Epoch 18/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0864 - acc: 0.9795 - val_loss: 0.3042 - val_acc: 0.9223\n",
      "Epoch 19/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0775 - acc: 0.9798 - val_loss: 0.2902 - val_acc: 0.9258\n",
      "Epoch 20/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0679 - acc: 0.9823 - val_loss: 0.2814 - val_acc: 0.9249\n",
      "Epoch 21/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0752 - acc: 0.9811 - val_loss: 0.3214 - val_acc: 0.9231\n",
      "Epoch 22/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0797 - acc: 0.9783 - val_loss: 0.2683 - val_acc: 0.9214\n",
      "Epoch 23/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0734 - acc: 0.9809 - val_loss: 0.3126 - val_acc: 0.9196\n",
      "Epoch 24/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0800 - acc: 0.9814 - val_loss: 0.2859 - val_acc: 0.9249\n",
      "Epoch 25/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0796 - acc: 0.9794 - val_loss: 0.3076 - val_acc: 0.9205\n",
      "Epoch 26/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0701 - acc: 0.9788 - val_loss: 0.3053 - val_acc: 0.9258\n",
      "Epoch 27/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0807 - acc: 0.9801 - val_loss: 0.3320 - val_acc: 0.9143\n",
      "Epoch 28/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0852 - acc: 0.9787 - val_loss: 0.3483 - val_acc: 0.9064\n",
      "Epoch 29/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0771 - acc: 0.9793 - val_loss: 0.3354 - val_acc: 0.9117\n",
      "Epoch 30/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0811 - acc: 0.9786 - val_loss: 0.3073 - val_acc: 0.9134\n",
      "Epoch 31/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0749 - acc: 0.9807 - val_loss: 0.3170 - val_acc: 0.9134\n",
      "Epoch 32/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0798 - acc: 0.9798 - val_loss: 0.3257 - val_acc: 0.9205\n",
      "Epoch 33/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0811 - acc: 0.9802 - val_loss: 0.3238 - val_acc: 0.9258\n",
      "Epoch 34/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0883 - acc: 0.9773 - val_loss: 0.2889 - val_acc: 0.9187\n",
      "Epoch 35/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0781 - acc: 0.9791 - val_loss: 0.3037 - val_acc: 0.9267\n",
      "Epoch 36/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0847 - acc: 0.9804 - val_loss: 0.2891 - val_acc: 0.9276\n",
      "Epoch 37/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0739 - acc: 0.9819 - val_loss: 0.3086 - val_acc: 0.9214\n",
      "Epoch 38/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0810 - acc: 0.9802 - val_loss: 0.3639 - val_acc: 0.9152\n",
      "Epoch 39/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0716 - acc: 0.9823 - val_loss: 0.3252 - val_acc: 0.9223\n",
      "Epoch 40/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0688 - acc: 0.9807 - val_loss: 0.3177 - val_acc: 0.9196\n",
      "Epoch 41/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0773 - acc: 0.9803 - val_loss: 0.4242 - val_acc: 0.9099\n",
      "Epoch 42/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0812 - acc: 0.9798 - val_loss: 0.3400 - val_acc: 0.9205\n",
      "Epoch 43/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0712 - acc: 0.9832 - val_loss: 0.3120 - val_acc: 0.9231\n",
      "Epoch 44/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0758 - acc: 0.9817 - val_loss: 0.2909 - val_acc: 0.9249\n",
      "Epoch 45/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0854 - acc: 0.9795 - val_loss: 0.3064 - val_acc: 0.9161\n",
      "Epoch 46/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0727 - acc: 0.9817 - val_loss: 0.3246 - val_acc: 0.9196\n",
      "Epoch 47/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0755 - acc: 0.9807 - val_loss: 0.3174 - val_acc: 0.9240\n",
      "Epoch 48/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0712 - acc: 0.9820 - val_loss: 0.3483 - val_acc: 0.9205\n",
      "Epoch 49/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0805 - acc: 0.9802 - val_loss: 0.3182 - val_acc: 0.9196\n",
      "Epoch 50/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0794 - acc: 0.9800 - val_loss: 0.3052 - val_acc: 0.9196\n",
      "Epoch 51/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0803 - acc: 0.9809 - val_loss: 0.3535 - val_acc: 0.9231\n",
      "Epoch 52/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0806 - acc: 0.9808 - val_loss: 0.3346 - val_acc: 0.9178\n",
      "Epoch 53/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0814 - acc: 0.9815 - val_loss: 0.3445 - val_acc: 0.9223\n",
      "Epoch 54/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0801 - acc: 0.9822 - val_loss: 0.3307 - val_acc: 0.9223\n",
      "Epoch 55/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0781 - acc: 0.9793 - val_loss: 0.3216 - val_acc: 0.9196\n",
      "Epoch 56/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0740 - acc: 0.9807 - val_loss: 0.3170 - val_acc: 0.9293\n",
      "Epoch 57/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0781 - acc: 0.9804 - val_loss: 0.3405 - val_acc: 0.9205\n",
      "Epoch 58/60\n",
      "10182/10182 [==============================] - 4870s - loss: 0.0750 - acc: 0.9826 - val_loss: 0.3104 - val_acc: 0.9231\n",
      "Epoch 59/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0812 - acc: 0.9798 - val_loss: 0.3044 - val_acc: 0.9231\n",
      "Epoch 60/60\n",
      "10182/10182 [==============================] - 136s - loss: 0.0864 - acc: 0.9800 - val_loss: 0.3151 - val_acc: 0.9214\n",
      "7532/7532 [==============================] - 37s    \n",
      "\n",
      "Test score: 1.07396375785 Test accuracy: 0.806425916629\n"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('20newsgroups') # 20newsgroups reuters\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_sequences(docs_train, y_train, docs_test, y_test)\n",
    "print(len(target_names), 'targets\\n')\n",
    "\n",
    "model = load_model('Model/news_cnn_300_1.h5') #news_cnn_300_1\n",
    "model.fit(x_train_m, y_train_m,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          validation_data = (x_test_m, y_test_m))\n",
    "model.save('Model/news_cnn_300_2.h5')\n",
    "\n",
    "# test the trained model\n",
    "score = model.evaluate(x_test_m, y_test_m, batch_size = 128, verbose = 1)\n",
    "print('\\nTest score:', score[0], 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 CNN Model Trained With Reuter Dataset and 300 Dim Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 test documents are loaded.\n",
      "\n",
      "Found 30976 unique tokens.\n",
      "Shape of x_train_m: (8982, 2000)\n",
      "Shape of y_train_m: (8982, 46)\n",
      "Shape of x_test_m: (2246, 2000)\n",
      "Shape of y_test_m: (2246, 46)\n",
      "46 targets\n",
      "\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/60\n",
      "8982/8982 [==============================] - 139s - loss: 2.2576 - acc: 0.4256 - val_loss: 2.1393 - val_acc: 0.4751\n",
      "Epoch 2/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.9312 - acc: 0.4925 - val_loss: 2.0923 - val_acc: 0.5316\n",
      "Epoch 3/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.8132 - acc: 0.5372 - val_loss: 1.9852 - val_acc: 0.5018\n",
      "Epoch 4/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.7160 - acc: 0.5677 - val_loss: 1.9751 - val_acc: 0.6033\n",
      "Epoch 5/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.6263 - acc: 0.5952 - val_loss: 1.7106 - val_acc: 0.5997\n",
      "Epoch 6/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.5372 - acc: 0.6155 - val_loss: 1.7213 - val_acc: 0.5690\n",
      "Epoch 7/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.4701 - acc: 0.6326 - val_loss: 1.5881 - val_acc: 0.6224\n",
      "Epoch 8/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.3775 - acc: 0.6533 - val_loss: 1.5146 - val_acc: 0.6647\n",
      "Epoch 9/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.3079 - acc: 0.6698 - val_loss: 1.5291 - val_acc: 0.6701\n",
      "Epoch 10/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.2546 - acc: 0.6896 - val_loss: 1.5407 - val_acc: 0.6273\n",
      "Epoch 11/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.1916 - acc: 0.6993 - val_loss: 1.4554 - val_acc: 0.6923\n",
      "Epoch 12/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.1450 - acc: 0.7130 - val_loss: 1.4066 - val_acc: 0.7110\n",
      "Epoch 13/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.1066 - acc: 0.7206 - val_loss: 1.3758 - val_acc: 0.7004\n",
      "Epoch 14/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.0548 - acc: 0.7291 - val_loss: 1.3834 - val_acc: 0.6750\n",
      "Epoch 15/60\n",
      "8982/8982 [==============================] - 126s - loss: 1.0291 - acc: 0.7356 - val_loss: 1.4877 - val_acc: 0.6728\n",
      "Epoch 16/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.9906 - acc: 0.7444 - val_loss: 1.3557 - val_acc: 0.6901\n",
      "Epoch 17/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.9521 - acc: 0.7496 - val_loss: 1.3578 - val_acc: 0.6923\n",
      "Epoch 18/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.9191 - acc: 0.7597 - val_loss: 1.3706 - val_acc: 0.6861\n",
      "Epoch 19/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.9090 - acc: 0.7619 - val_loss: 1.3225 - val_acc: 0.6959\n",
      "Epoch 20/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.8663 - acc: 0.7675 - val_loss: 1.2750 - val_acc: 0.7128\n",
      "Epoch 21/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.8361 - acc: 0.7730 - val_loss: 1.3211 - val_acc: 0.6857\n",
      "Epoch 22/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.8228 - acc: 0.7788 - val_loss: 1.2051 - val_acc: 0.7284\n",
      "Epoch 23/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.7946 - acc: 0.7876 - val_loss: 1.4365 - val_acc: 0.6621\n",
      "Epoch 24/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.7805 - acc: 0.7905 - val_loss: 1.1831 - val_acc: 0.7364\n",
      "Epoch 25/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.7564 - acc: 0.7937 - val_loss: 1.5174 - val_acc: 0.6256\n",
      "Epoch 26/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.7195 - acc: 0.8008 - val_loss: 1.1243 - val_acc: 0.7476\n",
      "Epoch 27/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.7195 - acc: 0.8034 - val_loss: 1.2845 - val_acc: 0.6955\n",
      "Epoch 28/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.6881 - acc: 0.8106 - val_loss: 1.2137 - val_acc: 0.7213\n",
      "Epoch 29/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.6748 - acc: 0.8125 - val_loss: 1.3516 - val_acc: 0.6959\n",
      "Epoch 30/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.6636 - acc: 0.8142 - val_loss: 1.1605 - val_acc: 0.7306\n",
      "Epoch 31/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.6468 - acc: 0.8213 - val_loss: 1.1868 - val_acc: 0.7257\n",
      "Epoch 32/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.6381 - acc: 0.8228 - val_loss: 1.4126 - val_acc: 0.6594\n",
      "Epoch 33/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.6251 - acc: 0.8284 - val_loss: 1.1512 - val_acc: 0.7386\n",
      "Epoch 34/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.5996 - acc: 0.8313 - val_loss: 1.2642 - val_acc: 0.7017\n",
      "Epoch 35/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.5988 - acc: 0.8357 - val_loss: 1.2656 - val_acc: 0.6990\n",
      "Epoch 36/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.5853 - acc: 0.8354 - val_loss: 1.1521 - val_acc: 0.7369\n",
      "Epoch 37/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.5549 - acc: 0.8422 - val_loss: 1.2162 - val_acc: 0.7142\n",
      "Epoch 38/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.5545 - acc: 0.8466 - val_loss: 1.0862 - val_acc: 0.7520\n",
      "Epoch 39/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.5399 - acc: 0.8451 - val_loss: 1.1014 - val_acc: 0.7538\n",
      "Epoch 40/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.5212 - acc: 0.8501 - val_loss: 1.1709 - val_acc: 0.7262\n",
      "Epoch 41/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.5366 - acc: 0.8473 - val_loss: 1.1684 - val_acc: 0.7284\n",
      "Epoch 42/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.5064 - acc: 0.8535 - val_loss: 1.2039 - val_acc: 0.7124\n",
      "Epoch 43/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.5207 - acc: 0.8546 - val_loss: 1.1466 - val_acc: 0.7404\n",
      "Epoch 44/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.4908 - acc: 0.8607 - val_loss: 1.1497 - val_acc: 0.7395\n",
      "Epoch 45/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.4810 - acc: 0.8626 - val_loss: 1.1461 - val_acc: 0.7480\n",
      "Epoch 46/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.4785 - acc: 0.8638 - val_loss: 1.1235 - val_acc: 0.7547\n",
      "Epoch 47/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.4778 - acc: 0.8611 - val_loss: 1.1419 - val_acc: 0.7391\n",
      "Epoch 48/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.4645 - acc: 0.8629 - val_loss: 1.2382 - val_acc: 0.7097\n",
      "Epoch 49/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.4633 - acc: 0.8684 - val_loss: 1.1570 - val_acc: 0.7400\n",
      "Epoch 50/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.4605 - acc: 0.8703 - val_loss: 1.0777 - val_acc: 0.7609\n",
      "Epoch 51/60\n",
      "8982/8982 [==============================] - 3976s - loss: 0.4631 - acc: 0.8696 - val_loss: 1.2432 - val_acc: 0.7142\n",
      "Epoch 52/60\n",
      "8982/8982 [==============================] - 126s - loss: 0.4498 - acc: 0.8749 - val_loss: 1.1594 - val_acc: 0.7386\n",
      "Epoch 53/60\n",
      "3584/8982 [==========>...................] - ETA: 69s - loss: 0.4434 - acc: 0.8730"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('reuters') # 20newsgroups reuters\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_sequences(docs_train, y_train, docs_test, y_test)\n",
    "print(len(target_names), 'targets\\n')\n",
    "\n",
    "# round 1 - epochs: 60, accuracy: \n",
    "# round 2 - epochs:\n",
    "model = load_model('Model/reuter_cnn_300_1.h5') #reuter_cnn_300_1\n",
    "model.fit(x_train_m, y_train_m,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          validation_data = (x_test_m, y_test_m))\n",
    "model.save('Model/reuter_cnn_300_2.h5')\n",
    "\n",
    "# test the trained model\n",
    "score = model.evaluate(x_test_m, y_test_m, batch_size = 128, verbose = 1)\n",
    "print('\\nTest score:', score[0], 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 3 Doc2Vector\n",
    "Build the unsupervised model PV_DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Reuters and 20newsgroup Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 training documents are loaded.\n",
      "7532 testing documents are loaded.\n",
      "\n",
      "7769 training documents are loaded.\n",
      "3019 testing documents are loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[news_docs_train, news_docs_test, news_y_train, news_y_test, news_target_names] = load_dataset('20newsgroups') # reuters or 20newsgroups\n",
    "[reuters_docs_train, reuters_docs_test, reuters_y_train, reuters_y_test, reuters_target_names] = load_dataset('reuters') # reuters or 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 news group doc for training\n",
      "7532 news group doc for test\n",
      "7769 reuters doc for training\n",
      "3019 reuters doc for test\n"
     ]
    }
   ],
   "source": [
    "input_file = open('alldata.txt', 'w')\n",
    "\n",
    "id_ = 0\n",
    "for doc in news_docs_train:\n",
    "    doc_id = 'news_train_%i' % id_\n",
    "    id_ = id_ + 1\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    doc_tokens = ' '.join(tokens).lower()\n",
    "    doc_tokens = doc_tokens.encode('ascii', 'ignore')\n",
    "    input_file.write('%s %s\\n' % (doc_id, doc_tokens))\n",
    "print(id_, \"news group doc for training\")\n",
    "    \n",
    "id_ = 0\n",
    "for doc in news_docs_test:\n",
    "    doc_id = 'news_test_%i' % id_\n",
    "    id_ = id_ + 1\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    doc_tokens = ' '.join(tokens).lower()\n",
    "    doc_tokens = doc_tokens.encode('ascii', 'ignore')\n",
    "    input_file.write('%s %s\\n' % (doc_id, doc_tokens))\n",
    "print(id_, \"news group doc for test\")\n",
    "    \n",
    "id_ = 0\n",
    "for doc in reuters_docs_train:\n",
    "    doc_id = 'reuters_train_%i' % id_\n",
    "    id_ = id_ + 1\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    doc_tokens = ' '.join(tokens).lower()\n",
    "    doc_tokens = doc_tokens.encode('ascii', 'ignore')\n",
    "    input_file.write('%s %s\\n' % (doc_id, doc_tokens))\n",
    "print(id_, \"reuters doc for training\")\n",
    "    \n",
    "id_ = 0\n",
    "for doc in reuters_docs_test:\n",
    "    doc_id = 'reuters_test_%i' % id_\n",
    "    id_ = id_ + 1\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    doc_tokens = ' '.join(tokens).lower()\n",
    "    doc_tokens = doc_tokens.encode('ascii', 'ignore')\n",
    "    input_file.write('%s %s\\n' % (doc_id, doc_tokens))\n",
    "print(id_, \"reuters doc for test\")\n",
    "    \n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29634\n",
      "29634\n"
     ]
    }
   ],
   "source": [
    "docList = []\n",
    "\n",
    "for idx in range(len(news_docs_train)):\n",
    "    docList.append('news_train_' + str(idx))\n",
    "\n",
    "for idx in range(len(news_docs_test)):\n",
    "    docList.append('news_test_' + str(idx))\n",
    "\n",
    "for idx in range(len(reuters_docs_train)):\n",
    "    docList.append('reuters_train_' + str(idx))\n",
    "    \n",
    "for idx in range(len(reuters_docs_test)):\n",
    "    docList.append('reuters_test_' + str(idx))\n",
    "\n",
    "print(len(docList))\n",
    "docList = set(docList)\n",
    "print(len(docList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained documents Vectors\n",
      "29634 document vectors are loaded.\n",
      "11314 news training examples\n",
      "7532 news training examples\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "def load_pretrain_doc_voc():\n",
    "    print('Loading pretrained documents Vectors')\n",
    "    doc_vector = {}\n",
    "    f = open(os.path.join('Model', 'vectors300.txt'), \"r\") # vectors100.txt vectors300.txt\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in docList:\n",
    "            coefs = np.array([float(x) for x in values[1].split(',')])\n",
    "            doc_vector[word] = coefs\n",
    "    f.close()\n",
    "    return doc_vector\n",
    "\n",
    "doc_vector = load_pretrain_doc_voc()\n",
    "print('%s document vectors are loaded.' % len(doc_vector))\n",
    "\n",
    "news_x_train = []\n",
    "for idx in range(len(news_docs_train)):\n",
    "    news_x_train.append(doc_vector['news_train_' + str(idx)])\n",
    "print(len(news_x_train), 'news training examples')\n",
    "\n",
    "news_x_test = []\n",
    "for idx in range(len(news_docs_test)):\n",
    "    news_x_test.append(doc_vector['news_test_' + str(idx)])\n",
    "print(len(news_x_test), 'news training examples')\n",
    "\n",
    "print(len(news_x_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test pretrained document vector for document classification:\n",
      "RBF:Correct rate = 0.7006107275624005 When C = 8\n",
      "RBF:Correct rate = 0.7282262347318109 When C = 16\n",
      "RBF:Correct rate = 0.7338024429102497 When C = 32\n",
      "RBF:Correct rate = 0.7430961232076474 When C = 64\n",
      "RBF:Correct rate = 0.7458842272968667 When C = 128\n",
      "RBF:Correct rate = 0.74429102496017 When C = 256\n",
      "Linear:Correct rate = 0.7138874137015401 When C = 0.01\n",
      "Linear:Correct rate = 0.7400424853956453 When C = 0.05\n",
      "Linear:Correct rate = 0.7497344662772172 When C = 0.5\n",
      "Linear:Correct rate = 0.7482740308019118 When C = 1.0\n",
      "Linear:Correct rate = 0.7458842272968667 When C = 2.0\n",
      "Linear:Correct rate = 0.739644184811471 When C = 4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting test pretrained document vector for document classification:\")\n",
    "X = news_x_train\n",
    "y = news_y_train\n",
    "C1 = [8, 16, 32, 64, 128, 256]\n",
    "X_test = news_x_test\n",
    "y_test = news_y_test\n",
    "C2 = [0.01, 0.05, 0.5, 1.0, 2.0, 4.0]\n",
    "svm_test(X, y, C1, X_test, y_test, C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3.2 Training Doc2Vec using gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['from', 'lerxst', 'wam', 'umd', 'edu', 'where', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst'], tags=['news_train_0']), TaggedDocument(words=['from', 'guykuo', 'carson', 'washington', 'edu', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'for', 'si', 'clock', 'reports', 'keywords', 'si', 'acceleration', 'clock', 'upgrade', 'article', 'shelley', 'qvfo', 'innc', 'organization', 'university', 'of', 'washington', 'lines', 'nntp', 'posting', 'host', 'carson', 'washington', 'edu', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'on', 'cards', 'and', 'adapters', 'heat', 'sinks', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', 'and', 'floppies', 'are', 'especially', 'requested', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', 'haven', 'answered', 'this', 'poll', 'thanks', 'guy', 'kuo', 'guykuo', 'washington', 'edu'], tags=['news_train_1'])]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Define a Function to Preprocess Text\n",
    "def create_training_data_set():\n",
    "    \n",
    "    train_corpus = []\n",
    "    for idx in range(len(news_docs_train)):\n",
    "        # For training data, add tags\n",
    "        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(news_docs_train[idx]),\n",
    "                                                                 ['news_train_' + str(idx)]))\n",
    "    \n",
    "    for idx in range(len(news_docs_test)):\n",
    "        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(news_docs_test[idx]),\n",
    "                                                                 ['news_test_' + str(idx)]))\n",
    "    \n",
    "    for idx in range(len(reuters_docs_train)):\n",
    "        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(reuters_docs_train[idx]),\n",
    "                                                                 ['reuters_train_' + str(idx)]))\n",
    "    \n",
    "    for idx in range(len(reuters_docs_test)):\n",
    "        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(reuters_docs_test[idx]),\n",
    "                                                                 ['reuters_test_' + str(idx)]))\n",
    "    return train_corpus\n",
    "\n",
    "# load training files and test files\n",
    "train_corpus = create_training_data_set()\n",
    "\n",
    "# show first two training data\n",
    "print(train_corpus[0])\n",
    "print(news_docs_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 31min 58s\n"
     ]
    }
   ],
   "source": [
    "# Define the Doc2Vec model\n",
    "# with a vector size with 50 words and iterating over the training corpus 55 times\n",
    "# set the minimum word count to 2 in order to give higher frequency words more weighting\n",
    "# Model accuracy can be improved by increasing the number of iterations\n",
    "model = gensim.models.doc2vec.Doc2Vec(dm = 1, dm_concat = 1, size=300, window=10, min_count=2, negative=10, iter=100, workers = 4)\n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save(\"Model/gensim300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 news training examples\n",
      "7532 news training examples\n",
      "Starting test document vector generated by gensim for document classification:\n",
      "RBF:Correct rate = 0.3332448220924057 When C = 0.5\n",
      "RBF:Correct rate = 0.35661178969729157 When C = 1.0\n",
      "RBF:Correct rate = 0.37121614445034523 When C = 2.0\n",
      "RBF:Correct rate = 0.38528943175783326 When C = 4.0\n",
      "RBF:Correct rate = 0.4013542219861922 When C = 8.0\n",
      "RBF:Correct rate = 0.40693043016463093 When C = 16\n",
      "RBF:Correct rate = 0.41768454593733406 When C = 32\n",
      "RBF:Correct rate = 0.42644715878916617 When C = 64\n",
      "RBF:Correct rate = 0.435740839086564 When C = 128\n",
      "RBF:Correct rate = 0.4403876792352629 When C = 256\n",
      "Linear:Correct rate = 0.5130111524163569 When C = 0.5\n",
      "Linear:Correct rate = 0.5289431757833245 When C = 1.0\n",
      "Linear:Correct rate = 0.5426181625066384 When C = 2.0\n",
      "Linear:Correct rate = 0.5524429102496017 When C = 4.0\n",
      "Linear:Correct rate = 0.5584174190122145 When C = 8.0\n",
      "Linear:Correct rate = 0.5606744556558683 When C = 16\n",
      "Linear:Correct rate = 0.5539033457249071 When C = 32\n",
      "Linear:Correct rate = 0.5416887944768985 When C = 64\n",
      "Linear:Correct rate = 0.4843335103558152 When C = 128\n",
      "Linear:Correct rate = 0.4472915560276155 When C = 256\n"
     ]
    }
   ],
   "source": [
    "# Test document vector generated by gensim for document classification\n",
    "# model = Doc2Vec.load(fname)\n",
    "doc_vector = model.docvecs\n",
    "\n",
    "news_x_train = []\n",
    "for idx in range(len(news_docs_train)):\n",
    "    news_x_train.append(doc_vector['news_train_' + str(idx)])\n",
    "print(len(news_x_train), 'news training examples')\n",
    "\n",
    "news_x_test = []\n",
    "for idx in range(len(news_docs_test)):\n",
    "    news_x_test.append(doc_vector['news_test_' + str(idx)])\n",
    "print(len(news_x_test), 'news training examples')\n",
    "\n",
    "print(\"Starting test document vector generated by gensim for document classification:\")\n",
    "X = news_x_train\n",
    "y = news_y_train\n",
    "X_test = news_x_test\n",
    "y_test = news_y_test\n",
    "svm_test(X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
