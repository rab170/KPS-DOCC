{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# using two datasets, they are 20newsgroups and Reuters-21578\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import random\n",
    "import json\n",
    "import collections\n",
    "from itertools import compress\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.datasets import reuters as reuters2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras import backend as K\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# import functions for draw the graph of models\n",
    "from IPython.display import SVG, display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import pydot\n",
    "# import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Loading Datasets and Classify Documents with Baselines\n",
    "\n",
    "Two baseline system is used: one is a multilayer perceptron and the other is support vector machine.\n",
    "Both of them classify documents using tf-idf vectors of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    docs_train = []\n",
    "    docs_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    target_names = []\n",
    "    \n",
    "    # using reuters dataset from keras package\n",
    "    if dataset_name == 'reuters':\n",
    "#         target_names = reuters.categories()\n",
    "#         for doc_id in reuters.fileids():\n",
    "#             file_target_list = reuters.categories(doc_id)\n",
    "#             if doc_id.startswith(\"train\"):\n",
    "#                 docs_train.append(reuters.raw(doc_id))\n",
    "#                 y = []\n",
    "#                 for file_target in file_target_list:                    \n",
    "#                     y.append(target_names.index(file_target))\n",
    "#                 y_train.append(y)\n",
    "#             else:\n",
    "#                 docs_test.append(reuters.raw(doc_id))\n",
    "#                 y = []\n",
    "#                 for file_target in file_target_list:\n",
    "#                     y.append(target_names.index(file_target))\n",
    "#                 y_test.append(y)\n",
    "        word_index = reuters2.get_word_index(path=\"reuters_word_index.json\")\n",
    "        inverse_word_dict = np.ndarray(shape=(len(word_index)+1,), dtype=object)\n",
    "        for key in word_index:\n",
    "            index = word_index[key]\n",
    "            inverse_word_dict[index] = key\n",
    "\n",
    "        print('Loading reuters dataset...')\n",
    "        (x_train, y_train), (x_test, y_test) = reuters2.load_data(test_split=0.2)\n",
    "        \n",
    "        for x in x_train:\n",
    "            x = [t for t in x if t < len(word_index)]\n",
    "            docs_train.append(' '.join(inverse_word_dict[x]))\n",
    "        print(len(docs_train), 'train docs', len(y_train))\n",
    "        \n",
    "        for x in x_test:\n",
    "            x = [t for t in x if t < len(word_index)]\n",
    "            docs_test.append(' '.join(inverse_word_dict[x]))\n",
    "        print(len(docs_test), 'test docs', len(y_test))\n",
    "        \n",
    "        target_names = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                        '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                        '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                        '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "                        '1', '2', '3', '4', '5', '6']\n",
    "    elif dataset_name == '20newsgroups':\n",
    "        newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "        docs_train = newsgroups_train.data\n",
    "        y_train = newsgroups_train.target\n",
    "        newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "        docs_test = newsgroups_test.data\n",
    "        y_test = newsgroups_test.target\n",
    "        target_names = newsgroups_train.target_names\n",
    "    \n",
    "    print(len(docs_train), \"training documents are loaded.\")\n",
    "    print(len(docs_test), \"test documents are loaded.\\n\")\n",
    "    \n",
    "    return docs_train, y_train, docs_test, y_test, np.array(target_names)\n",
    "\n",
    "# convert documents to bag of word vectors\n",
    "def doc_2_matrix(vocab_size, docs_train, y_train, docs_test, y_test):\n",
    "    \n",
    "    MAX_NB_WORDS = vocab_size\n",
    "    MAX_SEQUENCE_LENGTH = 2000\n",
    "    \n",
    "    # fit the tokenizer with the corpus\n",
    "    tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "    docs = []\n",
    "    docs.extend(docs_train)\n",
    "    docs.extend(docs_test)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    # vectorize texts into 2D integer tensors\n",
    "    # mode: \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\")\n",
    "    x_train_m = tokenizer.texts_to_matrix(docs_train, mode='tfidf')\n",
    "    y_train_m = to_categorical(np.asarray(y_train))\n",
    "    print('Shape of x_train_m:', x_train_m.shape)\n",
    "    print('Shape of y_train_m:', y_train_m.shape)\n",
    "    \n",
    "    x_test_m = tokenizer.texts_to_matrix(docs_test, mode='tfidf')\n",
    "    y_test_m = to_categorical(np.asarray(y_test))\n",
    "    print('Shape of x_test_m:', x_test_m.shape)\n",
    "    print('Shape of y_test_m:', y_test_m.shape)\n",
    "    \n",
    "    return x_train_m, y_train_m, x_test_m, y_test_m\n",
    "\n",
    "# convert documents to word index sequences\n",
    "def doc_2_sequences(docs_train, y_train, docs_test, y_test):\n",
    "    # vectorize the text samples into a 2D integer tensor\n",
    "    MAX_NB_WORDS = 60000\n",
    "    MAX_SEQUENCE_LENGTH = 2000\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "    docs = []\n",
    "    docs.extend(docs_train)\n",
    "    docs.extend(docs_test)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    x_train_m = pad_sequences(tokenizer.texts_to_sequences(docs_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    y_train_m = to_categorical(np.asarray(y_train))\n",
    "    print('Shape of x_train_m:', x_train_m.shape)\n",
    "    print('Shape of y_train_m:', y_train_m.shape)\n",
    "    \n",
    "    x_test_m = pad_sequences(tokenizer.texts_to_sequences(docs_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    y_test_m = to_categorical(np.asarray(y_test))\n",
    "    print('Shape of x_test_m:', x_test_m.shape)\n",
    "    print('Shape of y_test_m:', y_test_m.shape)\n",
    "    \n",
    "    return x_train_m, y_train_m, x_test_m, y_test_m\n",
    "\n",
    "class Swish(Activation):\n",
    "    \n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Swish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'swish'\n",
    "\n",
    "def swish(x):\n",
    "    return 0.9 * x * K.sigmoid(0.8015*x)\n",
    "\n",
    "get_custom_objects().update({'swish': Swish(swish)})\n",
    "\n",
    "def mlp_base_line(vocab_size, num_classes, x_train_m, y_train_m, x_test_m, y_test_m, epochs, dataset, isload, outfile):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    if isload == False:\n",
    "        print('Building a MLP baseline model...')\n",
    "        model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Dropout(0.75))\n",
    "        model.add(Dense(300))\n",
    "        model.add(Activation('swish'))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(num_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    else:\n",
    "        print('loading a pretrained MLP model...')\n",
    "        model = load_model('Model/' + dataset + outfile + '.h5')\n",
    "    \n",
    "    # show picture of the trained model\n",
    "    display(SVG(model_to_dot(model).create(prog='dot', format='svg')))\n",
    "    \n",
    "    batch_size = 128\n",
    "    history = model.fit(x_train_m, y_train_m,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        verbose = 1,\n",
    "                        validation_split = 0.1)\n",
    "    model.save('Model/' + dataset + outfile + '.h5')\n",
    "\n",
    "    score = model.evaluate(x_test_m, y_test_m, batch_size = batch_size, verbose = 1)\n",
    "    print('\\nTest score:', score[0], 'Test accuracy:', score[1])\n",
    "    \n",
    "def svm_test(X, y, C1, X_test, y_test, C2):\n",
    "    for c_value in C1:\n",
    "        clf = svm.SVC(C=c_value, gamma=1/len(X[0]))\n",
    "        start = time()\n",
    "        clf.fit(X, y)\n",
    "        end = time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        counter = 0;\n",
    "        for idx, pred in enumerate(y_pred):\n",
    "            if y_test[idx] != pred:\n",
    "                counter = counter + 1\n",
    "        print(\"RBF: Correct rate =\", 1 - counter/len(y_test), \" When C =\", c_value, \"Training time = \", (end - start))\n",
    "        \n",
    "    for c_value in C2:\n",
    "        lin_clf = svm.LinearSVC(C=c_value)\n",
    "        start = time()\n",
    "        lin_clf.fit(X, y)\n",
    "        end = time()\n",
    "        y_pred = lin_clf.predict(X_test)\n",
    "\n",
    "        counter = 0;\n",
    "        for idx, pred in enumerate(y_pred):\n",
    "            if y_test[idx] != pred:\n",
    "                counter = counter + 1\n",
    "        print(\"Linear: Correct rate =\", 1 - counter/len(y_test), \" When C =\", c_value, \"Training time = \", (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using MLP to classifiy documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 test documents are loaded.\n",
      "\n",
      "Found 30976 unique tokens.\n",
      "Shape of x_train_m: (8982, 30000)\n",
      "Shape of y_train_m: (8982, 46)\n",
      "Shape of x_test_m: (2246, 30000)\n",
      "Shape of y_test_m: (2246, 46)\n",
      "46 targets\n",
      "\n",
      "loading a pretrained MLP model...\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"629pt\" viewBox=\"0.00 0.00 174.00 629.00\" width=\"174pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 625)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-625 170,-625 170,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2906262338080 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2906262338080</title>\n",
       "<polygon fill=\"none\" points=\"0,-584.5 0,-620.5 166,-620.5 166,-584.5 0,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-598.8\">dense_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 2906262335616 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2906262335616</title>\n",
       "<polygon fill=\"none\" points=\"31,-511.5 31,-547.5 135,-547.5 135,-511.5 31,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-525.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 2906262338080&#45;&gt;2906262335616 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2906262338080-&gt;2906262335616</title>\n",
       "<path d=\"M83,-584.313C83,-576.289 83,-566.547 83,-557.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-557.529 83,-547.529 79.5001,-557.529 86.5001,-557.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2906262335672 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2906262335672</title>\n",
       "<polygon fill=\"none\" points=\"9,-438.5 9,-474.5 157,-474.5 157,-438.5 9,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-452.8\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 2906262335616&#45;&gt;2906262335672 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2906262335616-&gt;2906262335672</title>\n",
       "<path d=\"M83,-511.313C83,-503.289 83,-493.547 83,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-484.529 83,-474.529 79.5001,-484.529 86.5001,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2903098028440 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2903098028440</title>\n",
       "<polygon fill=\"none\" points=\"18,-365.5 18,-401.5 148,-401.5 148,-365.5 18,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-379.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 2906262335672&#45;&gt;2903098028440 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2906262335672-&gt;2903098028440</title>\n",
       "<path d=\"M83,-438.313C83,-430.289 83,-420.547 83,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-411.529 83,-401.529 79.5001,-411.529 86.5001,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2906262336400 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2906262336400</title>\n",
       "<polygon fill=\"none\" points=\"31,-292.5 31,-328.5 135,-328.5 135,-292.5 31,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-306.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 2903098028440&#45;&gt;2906262336400 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2903098028440-&gt;2906262336400</title>\n",
       "<path d=\"M83,-365.313C83,-357.289 83,-347.547 83,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-338.529 83,-328.529 79.5001,-338.529 86.5001,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2903098031184 -->\n",
       "<g class=\"node\" id=\"node6\"><title>2903098031184</title>\n",
       "<polygon fill=\"none\" points=\"9,-219.5 9,-255.5 157,-255.5 157,-219.5 9,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-233.8\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 2906262336400&#45;&gt;2903098031184 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>2906262336400-&gt;2903098031184</title>\n",
       "<path d=\"M83,-292.313C83,-284.289 83,-274.547 83,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-265.529 83,-255.529 79.5001,-265.529 86.5001,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2906140283288 -->\n",
       "<g class=\"node\" id=\"node7\"><title>2906140283288</title>\n",
       "<polygon fill=\"none\" points=\"18,-146.5 18,-182.5 148,-182.5 148,-146.5 18,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-160.8\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 2903098031184&#45;&gt;2906140283288 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>2903098031184-&gt;2906140283288</title>\n",
       "<path d=\"M83,-219.313C83,-211.289 83,-201.547 83,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-192.529 83,-182.529 79.5001,-192.529 86.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2903096845144 -->\n",
       "<g class=\"node\" id=\"node8\"><title>2903096845144</title>\n",
       "<polygon fill=\"none\" points=\"31,-73.5 31,-109.5 135,-109.5 135,-73.5 31,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-87.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 2906140283288&#45;&gt;2903096845144 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>2906140283288-&gt;2903096845144</title>\n",
       "<path d=\"M83,-146.313C83,-138.289 83,-128.547 83,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-119.529 83,-109.529 79.5001,-119.529 86.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2903043883920 -->\n",
       "<g class=\"node\" id=\"node9\"><title>2903043883920</title>\n",
       "<polygon fill=\"none\" points=\"9,-0.5 9,-36.5 157,-36.5 157,-0.5 9,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-14.8\">activation_3: Activation</text>\n",
       "</g>\n",
       "<!-- 2903096845144&#45;&gt;2903043883920 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>2903096845144-&gt;2903043883920</title>\n",
       "<path d=\"M83,-73.3129C83,-65.2895 83,-55.5475 83,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-46.5288 83,-36.5288 79.5001,-46.5289 86.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/6\n",
      "8083/8083 [==============================] - 19s 2ms/step - loss: 0.0300 - acc: 0.9899 - val_loss: 2.0687 - val_acc: 0.8087\n",
      "Epoch 2/6\n",
      "8083/8083 [==============================] - 18s 2ms/step - loss: 0.0263 - acc: 0.9904 - val_loss: 2.0978 - val_acc: 0.8065\n",
      "Epoch 3/6\n",
      "8083/8083 [==============================] - 18s 2ms/step - loss: 0.0209 - acc: 0.9926 - val_loss: 2.1088 - val_acc: 0.8042\n",
      "Epoch 4/6\n",
      "8083/8083 [==============================] - 18s 2ms/step - loss: 0.0206 - acc: 0.9934 - val_loss: 2.1275 - val_acc: 0.8065\n",
      "Epoch 5/6\n",
      "8083/8083 [==============================] - 18s 2ms/step - loss: 0.0203 - acc: 0.9938 - val_loss: 2.1575 - val_acc: 0.8042\n",
      "Epoch 6/6\n",
      "8083/8083 [==============================] - 18s 2ms/step - loss: 0.0176 - acc: 0.9939 - val_loss: 2.1672 - val_acc: 0.8076\n",
      "2246/2246 [==============================] - 2s 700us/step\n",
      "\n",
      "Test score: 2.06133476009 Test accuracy: 0.810774711021\n"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('reuters')\n",
    "vocab_size = 30000\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_matrix(vocab_size, docs_train, y_train, docs_test, y_test)\n",
    "print(len(target_names), 'targets\\n')\n",
    "mlp_base_line(x_train_m.shape[1], len(target_names), x_train_m, y_train_m, x_test_m, y_test_m, 6, 'reuters', True, 'MLP_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 training documents are loaded.\n",
      "7532 test documents are loaded.\n",
      "\n",
      "Found 179209 unique tokens.\n",
      "Shape of x_train_m: (11314, 30000)\n",
      "Shape of y_train_m: (11314, 20)\n",
      "Shape of x_test_m: (7532, 30000)\n",
      "Shape of y_test_m: (7532, 20)\n",
      "20 targets\n",
      "\n",
      "Building a MLP baseline model...\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"629pt\" viewBox=\"0.00 0.00 174.00 629.00\" width=\"174pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 625)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-625 170,-625 170,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1528891407944 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1528891407944</title>\n",
       "<polygon fill=\"none\" points=\"0,-584.5 0,-620.5 166,-620.5 166,-584.5 0,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-598.8\">dense_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1528891407776 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1528891407776</title>\n",
       "<polygon fill=\"none\" points=\"31,-511.5 31,-547.5 135,-547.5 135,-511.5 31,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-525.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 1528891407944&#45;&gt;1528891407776 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1528891407944-&gt;1528891407776</title>\n",
       "<path d=\"M83,-584.313C83,-576.289 83,-566.547 83,-557.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-557.529 83,-547.529 79.5001,-557.529 86.5001,-557.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1528840660584 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1528840660584</title>\n",
       "<polygon fill=\"none\" points=\"9,-438.5 9,-474.5 157,-474.5 157,-438.5 9,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-452.8\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 1528891407776&#45;&gt;1528840660584 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1528891407776-&gt;1528840660584</title>\n",
       "<path d=\"M83,-511.313C83,-503.289 83,-493.547 83,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-484.529 83,-474.529 79.5001,-484.529 86.5001,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1528840660192 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1528840660192</title>\n",
       "<polygon fill=\"none\" points=\"18,-365.5 18,-401.5 148,-401.5 148,-365.5 18,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-379.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 1528840660584&#45;&gt;1528840660192 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1528840660584-&gt;1528840660192</title>\n",
       "<path d=\"M83,-438.313C83,-430.289 83,-420.547 83,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-411.529 83,-401.529 79.5001,-411.529 86.5001,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1528840662040 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1528840662040</title>\n",
       "<polygon fill=\"none\" points=\"31,-292.5 31,-328.5 135,-328.5 135,-292.5 31,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-306.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 1528840660192&#45;&gt;1528840662040 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1528840660192-&gt;1528840662040</title>\n",
       "<path d=\"M83,-365.313C83,-357.289 83,-347.547 83,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-338.529 83,-328.529 79.5001,-338.529 86.5001,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1526902830976 -->\n",
       "<g class=\"node\" id=\"node6\"><title>1526902830976</title>\n",
       "<polygon fill=\"none\" points=\"9,-219.5 9,-255.5 157,-255.5 157,-219.5 9,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-233.8\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 1528840662040&#45;&gt;1526902830976 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>1528840662040-&gt;1526902830976</title>\n",
       "<path d=\"M83,-292.313C83,-284.289 83,-274.547 83,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-265.529 83,-255.529 79.5001,-265.529 86.5001,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1529030265152 -->\n",
       "<g class=\"node\" id=\"node7\"><title>1529030265152</title>\n",
       "<polygon fill=\"none\" points=\"18,-146.5 18,-182.5 148,-182.5 148,-146.5 18,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-160.8\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 1526902830976&#45;&gt;1529030265152 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>1526902830976-&gt;1529030265152</title>\n",
       "<path d=\"M83,-219.313C83,-211.289 83,-201.547 83,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-192.529 83,-182.529 79.5001,-192.529 86.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1529030263024 -->\n",
       "<g class=\"node\" id=\"node8\"><title>1529030263024</title>\n",
       "<polygon fill=\"none\" points=\"31,-73.5 31,-109.5 135,-109.5 135,-73.5 31,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-87.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 1529030265152&#45;&gt;1529030263024 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>1529030265152-&gt;1529030263024</title>\n",
       "<path d=\"M83,-146.313C83,-138.289 83,-128.547 83,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-119.529 83,-109.529 79.5001,-119.529 86.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1529030694280 -->\n",
       "<g class=\"node\" id=\"node9\"><title>1529030694280</title>\n",
       "<polygon fill=\"none\" points=\"9,-0.5 9,-36.5 157,-36.5 157,-0.5 9,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-14.8\">activation_3: Activation</text>\n",
       "</g>\n",
       "<!-- 1529030263024&#45;&gt;1529030694280 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>1529030263024-&gt;1529030694280</title>\n",
       "<path d=\"M83,-73.3129C83,-65.2895 83,-55.5475 83,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-46.5288 83,-36.5288 79.5001,-46.5289 86.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/60\n",
      "10182/10182 [==============================] - 25s 2ms/step - loss: 1.8074 - acc: 0.4772 - val_loss: 0.4179 - val_acc: 0.9055\n",
      "Epoch 2/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.5305 - acc: 0.8462 - val_loss: 0.2728 - val_acc: 0.9276\n",
      "Epoch 3/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.2781 - acc: 0.9177 - val_loss: 0.2327 - val_acc: 0.9311\n",
      "Epoch 4/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.1720 - acc: 0.9492 - val_loss: 0.2289 - val_acc: 0.9320\n",
      "Epoch 5/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.1211 - acc: 0.9633 - val_loss: 0.2409 - val_acc: 0.9329\n",
      "Epoch 6/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0907 - acc: 0.9733 - val_loss: 0.2438 - val_acc: 0.9276\n",
      "Epoch 7/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0719 - acc: 0.9788 - val_loss: 0.2537 - val_acc: 0.9302\n",
      "Epoch 8/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0674 - acc: 0.9808 - val_loss: 0.2607 - val_acc: 0.9293\n",
      "Epoch 9/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0563 - acc: 0.9840 - val_loss: 0.2522 - val_acc: 0.9276\n",
      "Epoch 10/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0493 - acc: 0.9870 - val_loss: 0.2618 - val_acc: 0.9302\n",
      "Epoch 11/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0433 - acc: 0.9881 - val_loss: 0.2739 - val_acc: 0.9267\n",
      "Epoch 12/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0433 - acc: 0.9871 - val_loss: 0.2809 - val_acc: 0.9284\n",
      "Epoch 13/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0310 - acc: 0.9906 - val_loss: 0.2906 - val_acc: 0.9284\n",
      "Epoch 14/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0363 - acc: 0.9906 - val_loss: 0.2977 - val_acc: 0.9329\n",
      "Epoch 15/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0356 - acc: 0.9904 - val_loss: 0.3018 - val_acc: 0.9293\n",
      "Epoch 16/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0282 - acc: 0.9924 - val_loss: 0.3293 - val_acc: 0.9240\n",
      "Epoch 17/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0309 - acc: 0.9918 - val_loss: 0.3203 - val_acc: 0.9240\n",
      "Epoch 18/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0236 - acc: 0.9933 - val_loss: 0.3166 - val_acc: 0.9293\n",
      "Epoch 19/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0280 - acc: 0.9930 - val_loss: 0.3140 - val_acc: 0.9302\n",
      "Epoch 20/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0286 - acc: 0.9932 - val_loss: 0.3106 - val_acc: 0.9302\n",
      "Epoch 21/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0266 - acc: 0.9925 - val_loss: 0.3236 - val_acc: 0.9311\n",
      "Epoch 22/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0355 - acc: 0.9909 - val_loss: 0.3318 - val_acc: 0.9258\n",
      "Epoch 23/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0332 - acc: 0.9909 - val_loss: 0.3182 - val_acc: 0.9311\n",
      "Epoch 24/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0327 - acc: 0.9897 - val_loss: 0.3410 - val_acc: 0.9302\n",
      "Epoch 25/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0269 - acc: 0.9915 - val_loss: 0.3376 - val_acc: 0.9293\n",
      "Epoch 26/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0342 - acc: 0.9909 - val_loss: 0.3330 - val_acc: 0.9302\n",
      "Epoch 27/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0299 - acc: 0.9926 - val_loss: 0.3578 - val_acc: 0.9258\n",
      "Epoch 28/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0284 - acc: 0.9923 - val_loss: 0.3717 - val_acc: 0.9293\n",
      "Epoch 29/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0342 - acc: 0.9907 - val_loss: 0.3676 - val_acc: 0.9284\n",
      "Epoch 30/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0257 - acc: 0.9909 - val_loss: 0.3919 - val_acc: 0.9293\n",
      "Epoch 31/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0371 - acc: 0.9905 - val_loss: 0.3863 - val_acc: 0.9302\n",
      "Epoch 32/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0319 - acc: 0.9914 - val_loss: 0.3780 - val_acc: 0.9337\n",
      "Epoch 33/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0391 - acc: 0.9884 - val_loss: 0.4139 - val_acc: 0.9311\n",
      "Epoch 34/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0298 - acc: 0.9911 - val_loss: 0.4395 - val_acc: 0.9293\n",
      "Epoch 35/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0357 - acc: 0.9907 - val_loss: 0.4128 - val_acc: 0.9302\n",
      "Epoch 36/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0308 - acc: 0.9924 - val_loss: 0.4193 - val_acc: 0.9284\n",
      "Epoch 37/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0358 - acc: 0.9903 - val_loss: 0.4062 - val_acc: 0.9311\n",
      "Epoch 38/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0390 - acc: 0.9900 - val_loss: 0.3865 - val_acc: 0.9311\n",
      "Epoch 39/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0420 - acc: 0.9900 - val_loss: 0.4248 - val_acc: 0.9320\n",
      "Epoch 40/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0248 - acc: 0.9923 - val_loss: 0.4020 - val_acc: 0.9346\n",
      "Epoch 41/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0314 - acc: 0.9920 - val_loss: 0.4351 - val_acc: 0.9302\n",
      "Epoch 42/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0327 - acc: 0.9913 - val_loss: 0.4116 - val_acc: 0.9329\n",
      "Epoch 43/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0321 - acc: 0.9909 - val_loss: 0.4159 - val_acc: 0.9329\n",
      "Epoch 44/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0350 - acc: 0.9909 - val_loss: 0.4214 - val_acc: 0.9258\n",
      "Epoch 45/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0378 - acc: 0.9898 - val_loss: 0.4253 - val_acc: 0.9311\n",
      "Epoch 46/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0291 - acc: 0.9920 - val_loss: 0.4190 - val_acc: 0.9293\n",
      "Epoch 47/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0365 - acc: 0.9892 - val_loss: 0.4288 - val_acc: 0.9276\n",
      "Epoch 48/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0410 - acc: 0.9900 - val_loss: 0.4421 - val_acc: 0.9276\n",
      "Epoch 49/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0389 - acc: 0.9905 - val_loss: 0.4485 - val_acc: 0.9267\n",
      "Epoch 50/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0350 - acc: 0.9911 - val_loss: 0.4768 - val_acc: 0.9293\n",
      "Epoch 51/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0414 - acc: 0.9896 - val_loss: 0.4618 - val_acc: 0.9293\n",
      "Epoch 52/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0360 - acc: 0.9909 - val_loss: 0.4604 - val_acc: 0.9311\n",
      "Epoch 53/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0421 - acc: 0.9915 - val_loss: 0.4609 - val_acc: 0.9337\n",
      "Epoch 54/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0420 - acc: 0.9888 - val_loss: 0.4757 - val_acc: 0.9302\n",
      "Epoch 55/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0327 - acc: 0.9923 - val_loss: 0.5070 - val_acc: 0.9293\n",
      "Epoch 56/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0335 - acc: 0.9902 - val_loss: 0.4545 - val_acc: 0.9329\n",
      "Epoch 57/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0396 - acc: 0.9906 - val_loss: 0.4773 - val_acc: 0.9337\n",
      "Epoch 58/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0362 - acc: 0.9906 - val_loss: 0.4595 - val_acc: 0.9355\n",
      "Epoch 59/60\n",
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0420 - acc: 0.9897 - val_loss: 0.4635 - val_acc: 0.9293\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10182/10182 [==============================] - 23s 2ms/step - loss: 0.0420 - acc: 0.9887 - val_loss: 0.4602 - val_acc: 0.9302\n",
      "7532/7532 [==============================] - 5s 663us/step\n",
      "\n",
      "Test score: 1.22736538666 Test accuracy: 0.846123208059\n"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('20newsgroups') # 20newsgroups\n",
    "vocab_size = 30000\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_matrix(vocab_size, docs_train, y_train, docs_test, y_test)\n",
    "print(len(target_names), 'targets\\n')\n",
    "mlp_base_line(x_train_m.shape[1], len(target_names), x_train_m, y_train_m, x_test_m, y_test_m, 60, 'news', False, 'MLP_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SVM to classify documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 test documents are loaded.\n",
      "\n",
      "Found 30976 unique tokens.\n",
      "Shape of x_train_m: (8982, 30000)\n",
      "Shape of y_train_m: (8982, 46)\n",
      "Shape of x_test_m: (2246, 30000)\n",
      "Shape of y_test_m: (2246, 46)\n",
      "46 targets\n",
      "\n",
      "RBF: Correct rate = 0.8067675868210151  When C = 8 Training time =  5036.703392744064\n",
      "RBF: Correct rate = 0.8107747105966162  When C = 16 Training time =  5432.142902851105\n",
      "RBF: Correct rate = 0.8116651825467498  When C = 32 Training time =  5435.881803750992\n",
      "RBF: Correct rate = 0.8125556544968834  When C = 64 Training time =  4670.658353567123\n",
      "RBF: Correct rate = 0.8094390026714159  When C = 128 Training time =  4212.804310321808\n",
      "RBF: Correct rate = 0.8081032947462155  When C = 256 Training time =  4169.693519592285\n",
      "Linear: Correct rate = 0.8063223508459484  When C = 5e-05 Training time =  2.9477622509002686\n",
      "Linear: Correct rate = 0.8227960819234195  When C = 0.0001 Training time =  2.4633376598358154\n",
      "Linear: Correct rate = 0.8268032056990204  When C = 0.0005 Training time =  3.02994966506958\n",
      "Linear: Correct rate = 0.825912733748887  When C = 0.001 Training time =  3.8272945880889893\n",
      "Linear: Correct rate = 0.8147818343722173  When C = 0.01 Training time =  11.433829545974731\n",
      "Linear: Correct rate = 0.8076580587711487  When C = 0.05 Training time =  13.876609802246094\n",
      "Linear: Correct rate = 0.7969723953695459  When C = 0.5 Training time =  18.34837317466736\n",
      "Linear: Correct rate = 0.788512911843277  When C = 1.0 Training time =  19.065346479415894\n"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('reuters') # 20newsgroups reuters\n",
    "vocab_size = 30000\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_matrix(vocab_size, docs_train, y_train, docs_test, y_test)\n",
    "C1 = [8, 16, 32, 64, 128, 256] # RBF\n",
    "C2 = [0.00005, 0.0001, 0.0005, 0.001, 0.01, 0.05, 0.5, 1.0] # Linear\n",
    "print(len(target_names), 'targets\\n')\n",
    "svm_test(x_train_m, y_train, C1, x_test_m, y_test, C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 training documents are loaded.\n",
      "7532 test documents are loaded.\n",
      "\n",
      "Found 179209 unique tokens.\n",
      "Shape of x_train_m: (11314, 30000)\n",
      "Shape of y_train_m: (11314, 20)\n",
      "Shape of x_test_m: (7532, 30000)\n",
      "Shape of y_test_m: (7532, 20)\n",
      "20 targets\n",
      "\n",
      "RBF: Correct rate = 0.8167817312798725  When C = 8 Training time =  4681.928590774536\n",
      "RBF: Correct rate = 0.8230217737652682  When C = 16 Training time =  4656.582355499268\n",
      "RBF: Correct rate = 0.8193043016463091  When C = 32 Training time =  4678.8241930007935\n",
      "RBF: Correct rate = 0.8194370685077005  When C = 64 Training time =  4537.1960327625275\n",
      "RBF: Correct rate = 0.8185077004779607  When C = 128 Training time =  4282.653331756592\n",
      "RBF: Correct rate = 0.8187732342007434  When C = 256 Training time =  3971.236307859421\n",
      "Linear: Correct rate = 0.8572756240042485  When C = 5e-05 Training time =  3.735177516937256\n",
      "Linear: Correct rate = 0.8596654275092936  When C = 0.0001 Training time =  3.1842267513275146\n",
      "Linear: Correct rate = 0.8570100902814657  When C = 0.0005 Training time =  3.1234724521636963\n",
      "Linear: Correct rate = 0.8511683483802444  When C = 0.001 Training time =  3.3038861751556396\n",
      "Linear: Correct rate = 0.8370950610727562  When C = 0.01 Training time =  6.225353240966797\n",
      "Linear: Correct rate = 0.8305894848645778  When C = 0.05 Training time =  9.055840492248535\n",
      "Linear: Correct rate = 0.8169144981412639  When C = 0.5 Training time =  12.217143535614014\n",
      "Linear: Correct rate = 0.8096123207647371  When C = 1.0 Training time =  11.452702283859253\n"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('20newsgroups') # 20newsgroups reuters\n",
    "vocab_size = 30000\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_matrix(vocab_size, docs_train, y_train, docs_test, y_test)\n",
    "C1 = [8, 16, 32, 64, 128, 256] # RBF\n",
    "C2 = [0.00005, 0.0001, 0.0005, 0.001, 0.01, 0.05, 0.5, 1.0] # Linear\n",
    "print(len(target_names), 'targets\\n')\n",
    "svm_test(x_train_m, y_train, C1, x_test_m, y_test, C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Recurrent Neural Network With Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Glove word embedding\n",
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'Dataset')\n",
    "vocab_size = 60000\n",
    "MAX_SEQUENCE_LENGTH = 2000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print('Loading the Glove word embedding')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.840B.300d.txt'), \"rb\") # glove.840B.300d   glove.6B.100d\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('UTF-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_RNN_model(dataset): # 20newsgroups reuters\n",
    "    [docs_train, y_train, docs_test, y_test, target_names] = load_dataset(dataset)\n",
    "    MAX_NB_WORDS = 60000\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "    docs = []\n",
    "    docs.extend(docs_train)\n",
    "    docs.extend(docs_test)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    print('Preparing embedding matrix')\n",
    "    num_words = min(vocab_size, len(word_index))\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    # set trainable = False to keep the embeddings fixed\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    print('Building RNN model')\n",
    "    drop_rate = 0.2\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    hidden_neurons = 200\n",
    "    model.add(LSTM(hidden_neurons, dropout=drop_rate, recurrent_dropout=drop_rate, return_sequences=False)) # activation='relu'\n",
    "    model.add(Dense(len(target_names), activation='softmax'))\n",
    "    # Choose the optimizer\n",
    "#     sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop', # sgd rmsprop\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    display(SVG(model_to_dot(model).create(prog='dot', format='svg')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 training documents are loaded.\n",
      "7532 test documents are loaded.\n",
      "\n",
      "Found 179209 unique tokens.\n",
      "Shape of x_train_m: (11314, 2000)\n",
      "Shape of y_train_m: (11314, 20)\n",
      "Shape of x_test_m: (7532, 2000)\n",
      "Shape of y_test_m: (7532, 20)\n",
      "20 targets\n",
      "\n",
      "11314 training documents are loaded.\n",
      "7532 test documents are loaded.\n",
      "\n",
      "Found 179209 unique tokens.\n",
      "Preparing embedding matrix\n",
      "Building RNN model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 2000, 300)         18000000  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                4020      \n",
      "=================================================================\n",
      "Total params: 18,404,820\n",
      "Trainable params: 404,820\n",
      "Non-trainable params: 18,000,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 203.00 264.00\" width=\"203pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 199,-260 199,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1922883324896 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1922883324896</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 195,-255.5 195,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-233.8\">embedding_2_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1922883333928 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1922883333928</title>\n",
       "<polygon fill=\"none\" points=\"16,-146.5 16,-182.5 179,-182.5 179,-146.5 16,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-160.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 1922883324896&#45;&gt;1922883333928 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1922883324896-&gt;1922883333928</title>\n",
       "<path d=\"M97.5,-219.313C97.5,-211.289 97.5,-201.547 97.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"101,-192.529 97.5,-182.529 94.0001,-192.529 101,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922883324448 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1922883324448</title>\n",
       "<polygon fill=\"none\" points=\"48.5,-73.5 48.5,-109.5 146.5,-109.5 146.5,-73.5 48.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-87.8\">lstm_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 1922883333928&#45;&gt;1922883324448 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1922883333928-&gt;1922883324448</title>\n",
       "<path d=\"M97.5,-146.313C97.5,-138.289 97.5,-128.547 97.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"101,-119.529 97.5,-109.529 94.0001,-119.529 101,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922886585872 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1922886585872</title>\n",
       "<polygon fill=\"none\" points=\"45.5,-0.5 45.5,-36.5 149.5,-36.5 149.5,-0.5 45.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 1922883324448&#45;&gt;1922886585872 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1922883324448-&gt;1922886585872</title>\n",
       "<path d=\"M97.5,-73.3129C97.5,-65.2895 97.5,-55.5475 97.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"101,-46.5288 97.5,-36.5288 94.0001,-46.5289 101,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 2000, 300)         18000000  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                4020      \n",
      "=================================================================\n",
      "Total params: 18,404,820\n",
      "Trainable params: 404,820\n",
      "Non-trainable params: 18,000,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 203.00 264.00\" width=\"203pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 199,-260 199,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1922886857784 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1922886857784</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 195,-255.5 195,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-233.8\">embedding_2_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1922886858176 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1922886858176</title>\n",
       "<polygon fill=\"none\" points=\"16,-146.5 16,-182.5 179,-182.5 179,-146.5 16,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-160.8\">embedding_2: Embedding</text>\n",
       "</g>\n",
       "<!-- 1922886857784&#45;&gt;1922886858176 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1922886857784-&gt;1922886858176</title>\n",
       "<path d=\"M97.5,-219.313C97.5,-211.289 97.5,-201.547 97.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"101,-192.529 97.5,-182.529 94.0001,-192.529 101,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922886857896 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1922886857896</title>\n",
       "<polygon fill=\"none\" points=\"48.5,-73.5 48.5,-109.5 146.5,-109.5 146.5,-73.5 48.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-87.8\">lstm_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 1922886858176&#45;&gt;1922886857896 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1922886858176-&gt;1922886857896</title>\n",
       "<path d=\"M97.5,-146.313C97.5,-138.289 97.5,-128.547 97.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"101,-119.529 97.5,-109.529 94.0001,-119.529 101,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922886847288 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1922886847288</title>\n",
       "<polygon fill=\"none\" points=\"45.5,-0.5 45.5,-36.5 149.5,-36.5 149.5,-0.5 45.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 1922886857896&#45;&gt;1922886847288 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1922886857896-&gt;1922886847288</title>\n",
       "<path d=\"M97.5,-73.3129C97.5,-65.2895 97.5,-55.5475 97.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"101,-46.5288 97.5,-36.5288 94.0001,-46.5289 101,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/100\n",
      "11314/11314 [==============================] - 1648s 146ms/step - loss: 0.9284 - acc: 0.0526 - val_loss: 1.1921e-07 - val_acc: 0.0424\n",
      "Epoch 2/100\n",
      "11314/11314 [==============================] - 1612s 142ms/step - loss: 1.1921e-07 - acc: 0.0424 - val_loss: 1.1921e-07 - val_acc: 0.0424\n",
      "Epoch 3/100\n",
      "11314/11314 [==============================] - 1625s 144ms/step - loss: 1.1921e-07 - acc: 0.0424 - val_loss: 1.1921e-07 - val_acc: 0.0424\n",
      "Epoch 4/100\n",
      "11314/11314 [==============================] - 1701s 150ms/step - loss: 1.1921e-07 - acc: 0.0424 - val_loss: 1.1921e-07 - val_acc: 0.0424\n",
      "Epoch 5/100\n",
      "11314/11314 [==============================] - 1651s 146ms/step - loss: 1.1921e-07 - acc: 0.0424 - val_loss: 1.1921e-07 - val_acc: 0.0424\n",
      "Epoch 6/100\n",
      "11314/11314 [==============================] - 1661s 147ms/step - loss: 1.1921e-07 - acc: 0.0424 - val_loss: 1.1921e-07 - val_acc: 0.0424\n",
      "Epoch 7/100\n",
      "11314/11314 [==============================] - 1630s 144ms/step - loss: 1.1921e-07 - acc: 0.0424 - val_loss: 1.1921e-07 - val_acc: 0.0424\n",
      "Epoch 8/100\n",
      "10112/11314 [=========================>....] - ETA: 2:33 - loss: 1.1921e-07 - acc: 0.0422"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-dde28f89b55b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m           validation_data = (x_test_m, y_test_m))\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model/news_rnn_300_2.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('20newsgroups') # 20newsgroups reuters\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_sequences(docs_train, y_train, docs_test, y_test)\n",
    "print(len(target_names), 'targets\\n')\n",
    "\n",
    "# model_20newsgroups = build_RNN_model('20newsgroups')\n",
    "# model_20newsgroups.save('Model/news_rnn_300_2.h5')\n",
    "\n",
    "# epochs/accuracy/TestAccuracy: 100/0.9267/0.75912;\n",
    "model = load_model('Model/news_rnn_300_2.h5')\n",
    "model.summary()\n",
    "display(SVG(model_to_dot(model).create(prog='dot', format='svg')))\n",
    "\n",
    "model.fit(x_train_m, y_train_m,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          validation_data = (x_test_m, y_test_m))\n",
    "model.save('Model/news_rnn_300_2.h5')\n",
    "\n",
    "# test the trained model\n",
    "score = model.evaluate(x_test_m, y_test_m, batch_size = 128, verbose = 1)\n",
    "print('\\nTest score:', score[0], 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 test documents are loaded.\n",
      "\n",
      "Found 30976 unique tokens.\n",
      "Shape of x_train_m: (8982, 2000)\n",
      "Shape of y_train_m: (8982, 46)\n",
      "Shape of x_test_m: (2246, 2000)\n",
      "Shape of y_test_m: (2246, 46)\n",
      "46 targets\n",
      "\n",
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 test documents are loaded.\n",
      "\n",
      "Found 30976 unique tokens.\n",
      "Preparing embedding matrix\n",
      "Building RNN model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2000, 300)         9292800   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                9246      \n",
      "=================================================================\n",
      "Total params: 9,702,846\n",
      "Trainable params: 410,046\n",
      "Non-trainable params: 9,292,800\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 203.00 264.00\" width=\"203pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 199,-260 199,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1926047172704 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1926047172704</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 195,-255.5 195,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-233.8\">embedding_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1926047055600 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1926047055600</title>\n",
       "<polygon fill=\"none\" points=\"16,-146.5 16,-182.5 179,-182.5 179,-146.5 16,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-160.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 1926047172704&#45;&gt;1926047055600 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1926047172704-&gt;1926047055600</title>\n",
       "<path d=\"M97.5,-219.313C97.5,-211.289 97.5,-201.547 97.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"101,-192.529 97.5,-182.529 94.0001,-192.529 101,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1926044783336 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1926044783336</title>\n",
       "<polygon fill=\"none\" points=\"48.5,-73.5 48.5,-109.5 146.5,-109.5 146.5,-73.5 48.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-87.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 1926047055600&#45;&gt;1926044783336 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1926047055600-&gt;1926044783336</title>\n",
       "<path d=\"M97.5,-146.313C97.5,-138.289 97.5,-128.547 97.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"101,-119.529 97.5,-109.529 94.0001,-119.529 101,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1926027347112 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1926027347112</title>\n",
       "<polygon fill=\"none\" points=\"45.5,-0.5 45.5,-36.5 149.5,-36.5 149.5,-0.5 45.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 1926044783336&#45;&gt;1926027347112 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1926044783336-&gt;1926027347112</title>\n",
       "<path d=\"M97.5,-73.3129C97.5,-65.2895 97.5,-55.5475 97.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"101,-46.5288 97.5,-36.5288 94.0001,-46.5289 101,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/100\n",
      "8982/8982 [==============================] - 1274s 142ms/step - loss: 0.1163 - acc: 0.0102 - val_loss: 1.1921e-07 - val_acc: 0.0053\n",
      "Epoch 2/100\n",
      "8982/8982 [==============================] - 1214s 135ms/step - loss: 1.1921e-07 - acc: 0.0061 - val_loss: 1.1921e-07 - val_acc: 0.0053\n",
      "Epoch 3/100\n",
      "8982/8982 [==============================] - 1241s 138ms/step - loss: 1.1921e-07 - acc: 0.0061 - val_loss: 1.1921e-07 - val_acc: 0.0053\n",
      "Epoch 4/100\n",
      " 128/8982 [..............................] - ETA: 18:59 - loss: 1.1921e-07 - acc: 0.0078"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0aae70933b96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m           validation_data = (x_test_m, y_test_m))\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model/reuter_rnn_300_2.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('reuters') # 20newsgroups reuters\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_sequences(docs_train, y_train, docs_test, y_test)\n",
    "print(len(target_names), 'targets\\n')\n",
    "\n",
    "model_reuters = build_RNN_model('reuters') # 20newsgroups reuters\n",
    "model_reuters.save('Model/reuter_rnn_300_2.h5')\n",
    "\n",
    "# epochs/accuracy/TestAccuracy: 100/0.9267/0.75912;\n",
    "model = load_model('Model/reuter_rnn_300_2.h5')\n",
    "model.summary()\n",
    "display(SVG(model_to_dot(model).create(prog='dot', format='svg')))\n",
    "\n",
    "model.fit(x_train_m, y_train_m,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          validation_data = (x_test_m, y_test_m))\n",
    "model.save('Model/reuter_rnn_300_2.h5')\n",
    "\n",
    "# test the trained model\n",
    "score = model.evaluate(x_test_m, y_test_m, batch_size = 128, verbose = 1)\n",
    "print('\\nTest score:', score[0], 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Convolutional Neural Network With Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_CNN_model(dataset): # 20newsgroups reuters\n",
    "    [docs_train, y_train, docs_test, y_test, target_names] = load_dataset(dataset)\n",
    "    MAX_NB_WORDS = 60000\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "    docs = []\n",
    "    docs.extend(docs_train)\n",
    "    docs.extend(docs_test)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    print('Preparing embedding matrix')\n",
    "    num_words = min(vocab_size, len(word_index))\n",
    "    print('num_words =', num_words)\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    # set trainable = False to keep the embeddings fixed\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    print('Building CNN model')\n",
    "    drop_rate = 0.6\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(5))\n",
    "    # rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(5))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    #model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(len(target_names), activation='softmax'))\n",
    "    # Choose the optimizer\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop', # sgd rmsprop\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    model.summary()\n",
    "    display(SVG(model_to_dot(model).create(prog='dot', format='svg')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 CNN Model Trained With 20 Newsgroup Dataset and 300 Dim Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 training documents are loaded.\n",
      "7532 test documents are loaded.\n",
      "\n",
      "Found 179209 unique tokens.\n",
      "Shape of x_train_m: (11314, 2000)\n",
      "Shape of y_train_m: (11314, 20)\n",
      "Shape of x_test_m: (7532, 2000)\n",
      "Shape of y_test_m: (7532, 20)\n",
      "20 targets\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 2000, 300)         18000000  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1996, 128)         192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 399, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 399, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 395, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 79, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 79, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 75, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 18,358,804\n",
      "Trainable params: 358,804\n",
      "Non-trainable params: 18,000,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"775pt\" viewBox=\"0.00 0.00 295.00 775.00\" width=\"295pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 771)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-771 291,-771 291,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1922041631352 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1922041631352</title>\n",
       "<polygon fill=\"none\" points=\"46,-730.5 46,-766.5 241,-766.5 241,-730.5 46,-730.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-744.8\">embedding_6_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1922041631576 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1922041631576</title>\n",
       "<polygon fill=\"none\" points=\"62,-657.5 62,-693.5 225,-693.5 225,-657.5 62,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-671.8\">embedding_6: Embedding</text>\n",
       "</g>\n",
       "<!-- 1922041631352&#45;&gt;1922041631576 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1922041631352-&gt;1922041631576</title>\n",
       "<path d=\"M143.5,-730.313C143.5,-722.289 143.5,-712.547 143.5,-703.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-703.529 143.5,-693.529 140,-703.529 147,-703.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922041702328 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1922041702328</title>\n",
       "<polygon fill=\"none\" points=\"80,-584.5 80,-620.5 207,-620.5 207,-584.5 80,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-598.8\">conv1d_7: Conv1D</text>\n",
       "</g>\n",
       "<!-- 1922041631576&#45;&gt;1922041702328 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1922041631576-&gt;1922041702328</title>\n",
       "<path d=\"M143.5,-657.313C143.5,-649.289 143.5,-639.547 143.5,-630.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-630.529 143.5,-620.529 140,-630.529 147,-630.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922041702608 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1922041702608</title>\n",
       "<polygon fill=\"none\" points=\"39,-511.5 39,-547.5 248,-547.5 248,-511.5 39,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-525.8\">max_pooling1d_5: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 1922041702328&#45;&gt;1922041702608 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1922041702328-&gt;1922041702608</title>\n",
       "<path d=\"M143.5,-584.313C143.5,-576.289 143.5,-566.547 143.5,-557.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-557.529 143.5,-547.529 140,-557.529 147,-557.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922041701264 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1922041701264</title>\n",
       "<polygon fill=\"none\" points=\"78.5,-438.5 78.5,-474.5 208.5,-474.5 208.5,-438.5 78.5,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-452.8\">dropout_7: Dropout</text>\n",
       "</g>\n",
       "<!-- 1922041702608&#45;&gt;1922041701264 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1922041702608-&gt;1922041701264</title>\n",
       "<path d=\"M143.5,-511.313C143.5,-503.289 143.5,-493.547 143.5,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-484.529 143.5,-474.529 140,-484.529 147,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922091074616 -->\n",
       "<g class=\"node\" id=\"node6\"><title>1922091074616</title>\n",
       "<polygon fill=\"none\" points=\"80,-365.5 80,-401.5 207,-401.5 207,-365.5 80,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-379.8\">conv1d_8: Conv1D</text>\n",
       "</g>\n",
       "<!-- 1922041701264&#45;&gt;1922091074616 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>1922041701264-&gt;1922091074616</title>\n",
       "<path d=\"M143.5,-438.313C143.5,-430.289 143.5,-420.547 143.5,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-411.529 143.5,-401.529 140,-411.529 147,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922085609032 -->\n",
       "<g class=\"node\" id=\"node7\"><title>1922085609032</title>\n",
       "<polygon fill=\"none\" points=\"39,-292.5 39,-328.5 248,-328.5 248,-292.5 39,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-306.8\">max_pooling1d_6: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 1922091074616&#45;&gt;1922085609032 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>1922091074616-&gt;1922085609032</title>\n",
       "<path d=\"M143.5,-365.313C143.5,-357.289 143.5,-347.547 143.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-338.529 143.5,-328.529 140,-338.529 147,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922041633872 -->\n",
       "<g class=\"node\" id=\"node8\"><title>1922041633872</title>\n",
       "<polygon fill=\"none\" points=\"78.5,-219.5 78.5,-255.5 208.5,-255.5 208.5,-219.5 78.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-233.8\">dropout_8: Dropout</text>\n",
       "</g>\n",
       "<!-- 1922085609032&#45;&gt;1922041633872 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>1922085609032-&gt;1922041633872</title>\n",
       "<path d=\"M143.5,-292.313C143.5,-284.289 143.5,-274.547 143.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-265.529 143.5,-255.529 140,-265.529 147,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922075601440 -->\n",
       "<g class=\"node\" id=\"node9\"><title>1922075601440</title>\n",
       "<polygon fill=\"none\" points=\"80,-146.5 80,-182.5 207,-182.5 207,-146.5 80,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-160.8\">conv1d_9: Conv1D</text>\n",
       "</g>\n",
       "<!-- 1922041633872&#45;&gt;1922075601440 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>1922041633872-&gt;1922075601440</title>\n",
       "<path d=\"M143.5,-219.313C143.5,-211.289 143.5,-201.547 143.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-192.529 143.5,-182.529 140,-192.529 147,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922074281632 -->\n",
       "<g class=\"node\" id=\"node10\"><title>1922074281632</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 287,-109.5 287,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-87.8\">global_max_pooling1d_3: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 1922075601440&#45;&gt;1922074281632 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>1922075601440-&gt;1922074281632</title>\n",
       "<path d=\"M143.5,-146.313C143.5,-138.289 143.5,-128.547 143.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-119.529 143.5,-109.529 140,-119.529 147,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1922074387456 -->\n",
       "<g class=\"node\" id=\"node11\"><title>1922074387456</title>\n",
       "<polygon fill=\"none\" points=\"91.5,-0.5 91.5,-36.5 195.5,-36.5 195.5,-0.5 91.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-14.8\">dense_8: Dense</text>\n",
       "</g>\n",
       "<!-- 1922074281632&#45;&gt;1922074387456 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>1922074281632-&gt;1922074387456</title>\n",
       "<path d=\"M143.5,-73.3129C143.5,-65.2895 143.5,-55.5475 143.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-46.5288 143.5,-36.5288 140,-46.5289 147,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0775 - acc: 0.9779 - val_loss: 0.8466 - val_acc: 0.8401\n",
      "Epoch 2/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0919 - acc: 0.9763 - val_loss: 0.8625 - val_acc: 0.8342\n",
      "Epoch 3/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0851 - acc: 0.9750 - val_loss: 0.8263 - val_acc: 0.8396\n",
      "Epoch 4/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0642 - acc: 0.9803 - val_loss: 0.9105 - val_acc: 0.8317\n",
      "Epoch 5/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0775 - acc: 0.9795 - val_loss: 0.8494 - val_acc: 0.8388\n",
      "Epoch 6/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0848 - acc: 0.9773 - val_loss: 0.8332 - val_acc: 0.8408\n",
      "Epoch 7/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0763 - acc: 0.9774 - val_loss: 0.9081 - val_acc: 0.8346\n",
      "Epoch 8/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0740 - acc: 0.9797 - val_loss: 0.8408 - val_acc: 0.8379\n",
      "Epoch 9/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0702 - acc: 0.9801 - val_loss: 0.8560 - val_acc: 0.8375\n",
      "Epoch 10/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0700 - acc: 0.9800 - val_loss: 0.8901 - val_acc: 0.8350\n",
      "Epoch 11/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0766 - acc: 0.9786 - val_loss: 0.8764 - val_acc: 0.8370\n",
      "Epoch 12/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0705 - acc: 0.9799 - val_loss: 0.8619 - val_acc: 0.8350\n",
      "Epoch 13/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0763 - acc: 0.9797 - val_loss: 0.8589 - val_acc: 0.8378\n",
      "Epoch 14/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0657 - acc: 0.9809 - val_loss: 0.9093 - val_acc: 0.8348\n",
      "Epoch 15/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0775 - acc: 0.9799 - val_loss: 0.8454 - val_acc: 0.8392\n",
      "Epoch 16/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0718 - acc: 0.9797 - val_loss: 0.8894 - val_acc: 0.8405\n",
      "Epoch 17/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0697 - acc: 0.9802 - val_loss: 0.8365 - val_acc: 0.8415\n",
      "Epoch 18/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0743 - acc: 0.9799 - val_loss: 0.9369 - val_acc: 0.8332\n",
      "Epoch 19/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0794 - acc: 0.9791 - val_loss: 0.8553 - val_acc: 0.8394\n",
      "Epoch 20/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0759 - acc: 0.9780 - val_loss: 0.8903 - val_acc: 0.8423\n",
      "Epoch 21/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0695 - acc: 0.9794 - val_loss: 0.8523 - val_acc: 0.8435\n",
      "Epoch 22/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0788 - acc: 0.9789 - val_loss: 0.8604 - val_acc: 0.8327\n",
      "Epoch 23/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0668 - acc: 0.9799 - val_loss: 0.8208 - val_acc: 0.8390\n",
      "Epoch 24/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0690 - acc: 0.9798 - val_loss: 0.8273 - val_acc: 0.8405\n",
      "Epoch 25/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0740 - acc: 0.9806 - val_loss: 0.8675 - val_acc: 0.8386\n",
      "Epoch 26/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0781 - acc: 0.9794 - val_loss: 0.8679 - val_acc: 0.8355\n",
      "Epoch 27/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0732 - acc: 0.9795 - val_loss: 0.8658 - val_acc: 0.8318\n",
      "Epoch 28/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0695 - acc: 0.9814 - val_loss: 0.8604 - val_acc: 0.8428\n",
      "Epoch 29/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0710 - acc: 0.9814 - val_loss: 0.9057 - val_acc: 0.8370\n",
      "Epoch 30/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0682 - acc: 0.9829 - val_loss: 0.9091 - val_acc: 0.8394\n",
      "Epoch 31/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0675 - acc: 0.9811 - val_loss: 0.8781 - val_acc: 0.8408\n",
      "Epoch 32/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0683 - acc: 0.9816 - val_loss: 0.9551 - val_acc: 0.8274\n",
      "Epoch 33/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0706 - acc: 0.9798 - val_loss: 0.9444 - val_acc: 0.8362\n",
      "Epoch 34/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0741 - acc: 0.9816 - val_loss: 0.8890 - val_acc: 0.8356\n",
      "Epoch 35/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0741 - acc: 0.9810 - val_loss: 0.8463 - val_acc: 0.8431\n",
      "Epoch 36/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0710 - acc: 0.9810 - val_loss: 0.9112 - val_acc: 0.8374\n",
      "Epoch 37/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0703 - acc: 0.9821 - val_loss: 0.8848 - val_acc: 0.8420\n",
      "Epoch 38/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0653 - acc: 0.9829 - val_loss: 0.9073 - val_acc: 0.8424\n",
      "Epoch 39/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0740 - acc: 0.9813 - val_loss: 0.9175 - val_acc: 0.8383\n",
      "Epoch 40/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0758 - acc: 0.9805 - val_loss: 0.9349 - val_acc: 0.8347\n",
      "Epoch 41/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0722 - acc: 0.9817 - val_loss: 0.9089 - val_acc: 0.8354\n",
      "Epoch 42/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0690 - acc: 0.9809 - val_loss: 0.8947 - val_acc: 0.8363\n",
      "Epoch 43/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0752 - acc: 0.9808 - val_loss: 0.9081 - val_acc: 0.8399\n",
      "Epoch 44/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0778 - acc: 0.9796 - val_loss: 0.9114 - val_acc: 0.8332\n",
      "Epoch 45/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0776 - acc: 0.9814 - val_loss: 0.8981 - val_acc: 0.8386\n",
      "Epoch 46/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0674 - acc: 0.9821 - val_loss: 0.8698 - val_acc: 0.8429\n",
      "Epoch 47/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0701 - acc: 0.9820 - val_loss: 0.8856 - val_acc: 0.8388\n",
      "Epoch 48/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0612 - acc: 0.9845 - val_loss: 0.8978 - val_acc: 0.8429\n",
      "Epoch 49/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0650 - acc: 0.9840 - val_loss: 0.9051 - val_acc: 0.8405\n",
      "Epoch 50/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0629 - acc: 0.9831 - val_loss: 0.9278 - val_acc: 0.8420\n",
      "Epoch 51/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0744 - acc: 0.9817 - val_loss: 0.9193 - val_acc: 0.8413\n",
      "Epoch 52/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0716 - acc: 0.9822 - val_loss: 0.9253 - val_acc: 0.8392\n",
      "Epoch 53/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0724 - acc: 0.9799 - val_loss: 0.8850 - val_acc: 0.8380\n",
      "Epoch 54/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0671 - acc: 0.9819 - val_loss: 0.9113 - val_acc: 0.8401\n",
      "Epoch 55/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0689 - acc: 0.9827 - val_loss: 0.8979 - val_acc: 0.8387\n",
      "Epoch 56/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0663 - acc: 0.9834 - val_loss: 0.9071 - val_acc: 0.8332\n",
      "Epoch 57/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0618 - acc: 0.9827 - val_loss: 0.8826 - val_acc: 0.8398\n",
      "Epoch 58/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0732 - acc: 0.9818 - val_loss: 0.9308 - val_acc: 0.8322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0624 - acc: 0.9822 - val_loss: 0.9094 - val_acc: 0.8366\n",
      "Epoch 60/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0667 - acc: 0.9838 - val_loss: 0.9748 - val_acc: 0.8303\n",
      "Epoch 61/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0881 - acc: 0.9795 - val_loss: 0.9488 - val_acc: 0.8370\n",
      "Epoch 62/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0656 - acc: 0.9842 - val_loss: 0.8959 - val_acc: 0.8407\n",
      "Epoch 63/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0751 - acc: 0.9801 - val_loss: 0.9449 - val_acc: 0.8370\n",
      "Epoch 64/100\n",
      "11314/11314 [==============================] - 184s 16ms/step - loss: 0.0692 - acc: 0.9815 - val_loss: 0.9364 - val_acc: 0.8403\n",
      "Epoch 65/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0600 - acc: 0.9834 - val_loss: 0.9697 - val_acc: 0.8366\n",
      "Epoch 66/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0681 - acc: 0.9843 - val_loss: 0.9227 - val_acc: 0.8370\n",
      "Epoch 67/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0708 - acc: 0.9833 - val_loss: 0.9530 - val_acc: 0.8350\n",
      "Epoch 68/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0633 - acc: 0.9838 - val_loss: 0.9923 - val_acc: 0.8319\n",
      "Epoch 69/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0554 - acc: 0.9847 - val_loss: 1.0392 - val_acc: 0.8277\n",
      "Epoch 70/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0588 - acc: 0.9839 - val_loss: 0.9682 - val_acc: 0.8367\n",
      "Epoch 71/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0720 - acc: 0.9827 - val_loss: 0.9390 - val_acc: 0.8372\n",
      "Epoch 72/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0598 - acc: 0.9827 - val_loss: 0.9747 - val_acc: 0.8334\n",
      "Epoch 73/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0694 - acc: 0.9829 - val_loss: 0.9505 - val_acc: 0.8401\n",
      "Epoch 74/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0638 - acc: 0.9836 - val_loss: 0.9513 - val_acc: 0.8348\n",
      "Epoch 75/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0626 - acc: 0.9841 - val_loss: 0.9643 - val_acc: 0.8383\n",
      "Epoch 76/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0679 - acc: 0.9832 - val_loss: 0.9383 - val_acc: 0.8376\n",
      "Epoch 77/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0703 - acc: 0.9822 - val_loss: 0.9849 - val_acc: 0.8372\n",
      "Epoch 78/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0637 - acc: 0.9830 - val_loss: 1.0246 - val_acc: 0.8324\n",
      "Epoch 79/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0601 - acc: 0.9845 - val_loss: 0.9626 - val_acc: 0.8383\n",
      "Epoch 80/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0671 - acc: 0.9825 - val_loss: 0.9777 - val_acc: 0.8362\n",
      "Epoch 81/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0637 - acc: 0.9835 - val_loss: 0.9641 - val_acc: 0.8368\n",
      "Epoch 82/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0600 - acc: 0.9845 - val_loss: 0.9859 - val_acc: 0.8336\n",
      "Epoch 83/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0665 - acc: 0.9824 - val_loss: 0.9587 - val_acc: 0.8408\n",
      "Epoch 84/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0643 - acc: 0.9836 - val_loss: 0.9752 - val_acc: 0.8383\n",
      "Epoch 85/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0603 - acc: 0.9854 - val_loss: 0.9558 - val_acc: 0.8391\n",
      "Epoch 86/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0705 - acc: 0.9839 - val_loss: 0.9546 - val_acc: 0.8423\n",
      "Epoch 87/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0659 - acc: 0.9829 - val_loss: 0.9793 - val_acc: 0.8384\n",
      "Epoch 88/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0723 - acc: 0.9823 - val_loss: 0.9658 - val_acc: 0.8360\n",
      "Epoch 89/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0715 - acc: 0.9824 - val_loss: 0.9694 - val_acc: 0.8352\n",
      "Epoch 90/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0717 - acc: 0.9837 - val_loss: 0.9689 - val_acc: 0.8401\n",
      "Epoch 91/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0745 - acc: 0.9818 - val_loss: 0.9672 - val_acc: 0.8415\n",
      "Epoch 92/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0682 - acc: 0.9829 - val_loss: 0.9413 - val_acc: 0.8387\n",
      "Epoch 93/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0750 - acc: 0.9831 - val_loss: 0.9597 - val_acc: 0.8401\n",
      "Epoch 94/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0619 - acc: 0.9835 - val_loss: 1.0010 - val_acc: 0.8371\n",
      "Epoch 95/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0689 - acc: 0.9849 - val_loss: 0.9796 - val_acc: 0.8400\n",
      "Epoch 96/100\n",
      "11314/11314 [==============================] - 184s 16ms/step - loss: 0.0628 - acc: 0.9840 - val_loss: 0.9077 - val_acc: 0.8457\n",
      "Epoch 97/100\n",
      "11314/11314 [==============================] - 184s 16ms/step - loss: 0.0572 - acc: 0.9839 - val_loss: 0.9219 - val_acc: 0.8431\n",
      "Epoch 98/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0618 - acc: 0.9852 - val_loss: 0.9503 - val_acc: 0.8429\n",
      "Epoch 99/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0702 - acc: 0.9835 - val_loss: 0.9587 - val_acc: 0.8401\n",
      "Epoch 100/100\n",
      "11314/11314 [==============================] - 185s 16ms/step - loss: 0.0595 - acc: 0.9852 - val_loss: 0.9553 - val_acc: 0.8395\n",
      "7532/7532 [==============================] - 38s 5ms/step\n",
      "\n",
      "Test score: 0.955333706868 Test accuracy: 0.839484864578\n"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('20newsgroups') # 20newsgroups\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_sequences(docs_train, y_train, docs_test, y_test)\n",
    "print(len(target_names), 'targets\\n')\n",
    "\n",
    "# model_20newsgroups = build_CNN_model('20newsgroups') # 20newsgroups reuters\n",
    "# model_20newsgroups.save('Model/news_cnn_300_2.h5')\n",
    "\n",
    "# epochs/accuracy/TestAccuracy: 100/0.9771/0.8341;\n",
    "model = load_model('Model/news_cnn_300_2.h5') #news_cnn_300_2\n",
    "model.summary()\n",
    "display(SVG(model_to_dot(model).create(prog='dot', format='svg')))\n",
    "\n",
    "model.fit(x_train_m, y_train_m,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          validation_data = (x_test_m, y_test_m))\n",
    "model.save('Model/news_cnn_300_2.h5')\n",
    "\n",
    "# test the trained model\n",
    "score = model.evaluate(x_test_m, y_test_m, batch_size = 128, verbose = 1)\n",
    "print('\\nTest score:', score[0], 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 CNN Model Trained With Reuter Dataset and 300 Dim Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters dataset...\n",
      "8982 train docs 8982\n",
      "2246 test docs 2246\n",
      "8982 training documents are loaded.\n",
      "2246 test documents are loaded.\n",
      "\n",
      "Found 30976 unique tokens.\n",
      "Shape of x_train_m: (8982, 2000)\n",
      "Shape of y_train_m: (8982, 46)\n",
      "Shape of x_test_m: (2246, 2000)\n",
      "Shape of y_test_m: (2246, 46)\n",
      "46 targets\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 2000, 300)         9292800   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1996, 128)         192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 399, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 399, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 395, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 79, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 79, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 9,654,958\n",
      "Trainable params: 9,654,958\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"775pt\" viewBox=\"0.00 0.00 295.00 775.00\" width=\"295pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 771)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-771 291,-771 291,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2631277862248 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2631277862248</title>\n",
       "<polygon fill=\"none\" points=\"46,-730.5 46,-766.5 241,-766.5 241,-730.5 46,-730.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-744.8\">embedding_3_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 2631277859056 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2631277859056</title>\n",
       "<polygon fill=\"none\" points=\"62,-657.5 62,-693.5 225,-693.5 225,-657.5 62,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-671.8\">embedding_3: Embedding</text>\n",
       "</g>\n",
       "<!-- 2631277862248&#45;&gt;2631277859056 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2631277862248-&gt;2631277859056</title>\n",
       "<path d=\"M143.5,-730.313C143.5,-722.289 143.5,-712.547 143.5,-703.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-703.529 143.5,-693.529 140,-703.529 147,-703.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2628677520128 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2628677520128</title>\n",
       "<polygon fill=\"none\" points=\"80,-584.5 80,-620.5 207,-620.5 207,-584.5 80,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-598.8\">conv1d_1: Conv1D</text>\n",
       "</g>\n",
       "<!-- 2631277859056&#45;&gt;2628677520128 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2631277859056-&gt;2628677520128</title>\n",
       "<path d=\"M143.5,-657.313C143.5,-649.289 143.5,-639.547 143.5,-630.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-630.529 143.5,-620.529 140,-630.529 147,-630.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2628677518784 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2628677518784</title>\n",
       "<polygon fill=\"none\" points=\"39,-511.5 39,-547.5 248,-547.5 248,-511.5 39,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-525.8\">max_pooling1d_1: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 2628677520128&#45;&gt;2628677518784 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2628677520128-&gt;2628677518784</title>\n",
       "<path d=\"M143.5,-584.313C143.5,-576.289 143.5,-566.547 143.5,-557.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-557.529 143.5,-547.529 140,-557.529 147,-557.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2628677247552 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2628677247552</title>\n",
       "<polygon fill=\"none\" points=\"78.5,-438.5 78.5,-474.5 208.5,-474.5 208.5,-438.5 78.5,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-452.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 2628677518784&#45;&gt;2628677247552 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2628677518784-&gt;2628677247552</title>\n",
       "<path d=\"M143.5,-511.313C143.5,-503.289 143.5,-493.547 143.5,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-484.529 143.5,-474.529 140,-484.529 147,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2630631220336 -->\n",
       "<g class=\"node\" id=\"node6\"><title>2630631220336</title>\n",
       "<polygon fill=\"none\" points=\"80,-365.5 80,-401.5 207,-401.5 207,-365.5 80,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-379.8\">conv1d_2: Conv1D</text>\n",
       "</g>\n",
       "<!-- 2628677247552&#45;&gt;2630631220336 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>2628677247552-&gt;2630631220336</title>\n",
       "<path d=\"M143.5,-438.313C143.5,-430.289 143.5,-420.547 143.5,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-411.529 143.5,-401.529 140,-411.529 147,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2628652784216 -->\n",
       "<g class=\"node\" id=\"node7\"><title>2628652784216</title>\n",
       "<polygon fill=\"none\" points=\"39,-292.5 39,-328.5 248,-328.5 248,-292.5 39,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-306.8\">max_pooling1d_2: MaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 2630631220336&#45;&gt;2628652784216 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>2630631220336-&gt;2628652784216</title>\n",
       "<path d=\"M143.5,-365.313C143.5,-357.289 143.5,-347.547 143.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-338.529 143.5,-328.529 140,-338.529 147,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2628645621712 -->\n",
       "<g class=\"node\" id=\"node8\"><title>2628645621712</title>\n",
       "<polygon fill=\"none\" points=\"78.5,-219.5 78.5,-255.5 208.5,-255.5 208.5,-219.5 78.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-233.8\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 2628652784216&#45;&gt;2628645621712 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>2628652784216-&gt;2628645621712</title>\n",
       "<path d=\"M143.5,-292.313C143.5,-284.289 143.5,-274.547 143.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-265.529 143.5,-255.529 140,-265.529 147,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2628645621264 -->\n",
       "<g class=\"node\" id=\"node9\"><title>2628645621264</title>\n",
       "<polygon fill=\"none\" points=\"80,-146.5 80,-182.5 207,-182.5 207,-146.5 80,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-160.8\">conv1d_3: Conv1D</text>\n",
       "</g>\n",
       "<!-- 2628645621712&#45;&gt;2628645621264 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>2628645621712-&gt;2628645621264</title>\n",
       "<path d=\"M143.5,-219.313C143.5,-211.289 143.5,-201.547 143.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-192.529 143.5,-182.529 140,-192.529 147,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2628676173328 -->\n",
       "<g class=\"node\" id=\"node10\"><title>2628676173328</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 287,-109.5 287,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-87.8\">global_max_pooling1d_1: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 2628645621264&#45;&gt;2628676173328 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>2628645621264-&gt;2628676173328</title>\n",
       "<path d=\"M143.5,-146.313C143.5,-138.289 143.5,-128.547 143.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-119.529 143.5,-109.529 140,-119.529 147,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2631142596448 -->\n",
       "<g class=\"node\" id=\"node11\"><title>2631142596448</title>\n",
       "<polygon fill=\"none\" points=\"91.5,-0.5 91.5,-36.5 195.5,-36.5 195.5,-0.5 91.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.5\" y=\"-14.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 2628676173328&#45;&gt;2631142596448 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>2628676173328-&gt;2631142596448</title>\n",
       "<path d=\"M143.5,-73.3129C143.5,-65.2895 143.5,-55.5475 143.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147,-46.5288 143.5,-36.5288 140,-46.5289 147,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/80\n",
      "8982/8982 [==============================] - 179s 20ms/step - loss: 0.1597 - acc: 0.9511 - val_loss: 1.5697 - val_acc: 0.7404\n",
      "Epoch 2/80\n",
      "8982/8982 [==============================] - 178s 20ms/step - loss: 0.1504 - acc: 0.9534 - val_loss: 1.6105 - val_acc: 0.7297\n",
      "Epoch 3/80\n",
      "8982/8982 [==============================] - 179s 20ms/step - loss: 0.1592 - acc: 0.9503 - val_loss: 1.6006 - val_acc: 0.7462\n",
      "Epoch 4/80\n",
      "8982/8982 [==============================] - 178s 20ms/step - loss: 0.1595 - acc: 0.9507 - val_loss: 1.6023 - val_acc: 0.7582\n",
      "Epoch 5/80\n",
      "8982/8982 [==============================] - 177s 20ms/step - loss: 0.1651 - acc: 0.9521 - val_loss: 1.6222 - val_acc: 0.7484\n",
      "Epoch 6/80\n",
      "8982/8982 [==============================] - 177s 20ms/step - loss: 0.1555 - acc: 0.9510 - val_loss: 1.6273 - val_acc: 0.7582\n",
      "Epoch 7/80\n",
      "8982/8982 [==============================] - 177s 20ms/step - loss: 0.1564 - acc: 0.9516 - val_loss: 1.5947 - val_acc: 0.7587\n",
      "Epoch 8/80\n",
      "8982/8982 [==============================] - 177s 20ms/step - loss: 0.1545 - acc: 0.9503 - val_loss: 1.6362 - val_acc: 0.7520\n",
      "Epoch 9/80\n",
      "8982/8982 [==============================] - 178s 20ms/step - loss: 0.1481 - acc: 0.9536 - val_loss: 1.7028 - val_acc: 0.7373\n",
      "Epoch 10/80\n",
      "8982/8982 [==============================] - 177s 20ms/step - loss: 0.1545 - acc: 0.9527 - val_loss: 1.6265 - val_acc: 0.7524\n",
      "Epoch 11/80\n",
      "8982/8982 [==============================] - 177s 20ms/step - loss: 0.1571 - acc: 0.9534 - val_loss: 1.6335 - val_acc: 0.7484\n",
      "Epoch 12/80\n",
      "8982/8982 [==============================] - 178s 20ms/step - loss: 0.1447 - acc: 0.9558 - val_loss: 1.6619 - val_acc: 0.7462\n",
      "Epoch 13/80\n",
      "2176/8982 [======>.......................] - ETA: 2:05 - loss: 0.1266 - acc: 0.9600"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,300,1,2000]\n\t [[Node: training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/Conv2DBackpropInput = Conv2DBackpropInput[T=DT_FLOAT, _class=[\"loc:@conv1d_1_1/convolution/Conv2D\"], data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/ShapeN, conv1d_1_1/convolution/ExpandDims_1, training_1/RMSprop/gradients/conv1d_1_1/convolution/Squeeze_grad/Reshape)]]\n\nCaused by op 'training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/Conv2DBackpropInput', defined at:\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-037dde95403b>\", line 9, in <module>\n    model = load_model('Model/reuter_cnn_300_2.h5') #reuter_cnn_300_2\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 276, in load_model\n    model.model._make_train_function()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 225, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2369, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 474, in _Conv2DGrad\n    data_format),\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 778, in conv2d_backprop_input\n    use_cudnn_on_gpu=use_cudnn_on_gpu, data_format=data_format, name=name)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'conv1d_1_1/convolution/Conv2D', defined at:\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 19 identical lines from previous traceback]\n  File \"<ipython-input-4-037dde95403b>\", line 9, in <module>\n    model = load_model('Model/reuter_cnn_300_2.h5') #reuter_cnn_300_2\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 239, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 313, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 139, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 1249, in from_config\n    model.add(layer)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 475, in add\n    output_tensor = layer(self.outputs[0])\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 156, in call\n    dilation_rate=self.dilation_rate[0])\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3147, in conv1d\n    data_format=tf_data_format)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 751, in convolution\n    return op(input, filter)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 835, in __call__\n    return self.conv_op(inp, filter)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 499, in __call__\n    return self.call(inp, filter)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 187, in __call__\n    name=self.name)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 177, in _conv1d\n    data_format=data_format, name=name)\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,300,1,2000]\n\t [[Node: training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/Conv2DBackpropInput = Conv2DBackpropInput[T=DT_FLOAT, _class=[\"loc:@conv1d_1_1/convolution/Conv2D\"], data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/ShapeN, conv1d_1_1/convolution/ExpandDims_1, training_1/RMSprop/gradients/conv1d_1_1/convolution/Squeeze_grad/Reshape)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,300,1,2000]\n\t [[Node: training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/Conv2DBackpropInput = Conv2DBackpropInput[T=DT_FLOAT, _class=[\"loc:@conv1d_1_1/convolution/Conv2D\"], data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/ShapeN, conv1d_1_1/convolution/ExpandDims_1, training_1/RMSprop/gradients/conv1d_1_1/convolution/Squeeze_grad/Reshape)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-037dde95403b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m           validation_data = (x_test_m, y_test_m))\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model/reuter_cnn_300_2.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,300,1,2000]\n\t [[Node: training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/Conv2DBackpropInput = Conv2DBackpropInput[T=DT_FLOAT, _class=[\"loc:@conv1d_1_1/convolution/Conv2D\"], data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/ShapeN, conv1d_1_1/convolution/ExpandDims_1, training_1/RMSprop/gradients/conv1d_1_1/convolution/Squeeze_grad/Reshape)]]\n\nCaused by op 'training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/Conv2DBackpropInput', defined at:\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-037dde95403b>\", line 9, in <module>\n    model = load_model('Model/reuter_cnn_300_2.h5') #reuter_cnn_300_2\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 276, in load_model\n    model.model._make_train_function()\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 225, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2369, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 474, in _Conv2DGrad\n    data_format),\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 778, in conv2d_backprop_input\n    use_cudnn_on_gpu=use_cudnn_on_gpu, data_format=data_format, name=name)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'conv1d_1_1/convolution/Conv2D', defined at:\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 19 identical lines from previous traceback]\n  File \"<ipython-input-4-037dde95403b>\", line 9, in <module>\n    model = load_model('Model/reuter_cnn_300_2.h5') #reuter_cnn_300_2\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 239, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 313, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 139, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 1249, in from_config\n    model.add(layer)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 475, in add\n    output_tensor = layer(self.outputs[0])\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 156, in call\n    dilation_rate=self.dilation_rate[0])\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3147, in conv1d\n    data_format=tf_data_format)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 751, in convolution\n    return op(input, filter)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 835, in __call__\n    return self.conv_op(inp, filter)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 499, in __call__\n    return self.call(inp, filter)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 187, in __call__\n    name=self.name)\n  File \"C:\\Users\\WangYong\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 177, in _conv1d\n    data_format=data_format, name=name)\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,300,1,2000]\n\t [[Node: training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/Conv2DBackpropInput = Conv2DBackpropInput[T=DT_FLOAT, _class=[\"loc:@conv1d_1_1/convolution/Conv2D\"], data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_1/RMSprop/gradients/conv1d_1_1/convolution/Conv2D_grad/ShapeN, conv1d_1_1/convolution/ExpandDims_1, training_1/RMSprop/gradients/conv1d_1_1/convolution/Squeeze_grad/Reshape)]]\n"
     ]
    }
   ],
   "source": [
    "[docs_train, y_train, docs_test, y_test, target_names] = load_dataset('reuters') # reuters\n",
    "x_train_m, y_train_m, x_test_m, y_test_m = doc_2_sequences(docs_train, y_train, docs_test, y_test)\n",
    "print(len(target_names), 'targets\\n')\n",
    "\n",
    "# model_reuters = build_CNN_model('reuters') # 20newsgroups reuters\n",
    "# model_reuters.save('Model/reuter_cnn_300_2.h5')\n",
    "\n",
    "# epochs/accuracy/TestAccuracy: 100/0.9529/0.7399; 180/0.9529/0.7399;\n",
    "model = load_model('Model/reuter_cnn_300_2.h5') #reuter_cnn_300_2\n",
    "model.summary()\n",
    "display(SVG(model_to_dot(model).create(prog='dot', format='svg')))\n",
    "\n",
    "model.fit(x_train_m, y_train_m,\n",
    "          batch_size=128,\n",
    "          epochs=80,\n",
    "          validation_data = (x_test_m, y_test_m))\n",
    "model.save('Model/reuter_cnn_300_2.h5')\n",
    "\n",
    "# test the trained model\n",
    "score = model.evaluate(x_test_m, y_test_m, batch_size = 128, verbose = 1)\n",
    "print('\\nTest score:', score[0], 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 4 Doc2Vector\n",
    "Build the unsupervised model PV_DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Reuters and 20newsgroup Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 training documents are loaded.\n",
      "7532 testing documents are loaded.\n",
      "\n",
      "7769 training documents are loaded.\n",
      "3019 testing documents are loaded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "[news_docs_train, news_docs_test, news_y_train, news_y_test, news_target_names] = load_dataset('20newsgroups') # reuters or 20newsgroups\n",
    "[reuters_docs_train, reuters_docs_test, reuters_y_train, reuters_y_test, reuters_target_names] = load_dataset('reuters') # reuters or 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 news group doc for training\n",
      "7532 news group doc for test\n",
      "7769 reuters doc for training\n",
      "3019 reuters doc for test\n"
     ]
    }
   ],
   "source": [
    "input_file = open('alldata.txt', 'w')\n",
    "\n",
    "id_ = 0\n",
    "for doc in news_docs_train:\n",
    "    doc_id = 'news_train_%i' % id_\n",
    "    id_ = id_ + 1\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    doc_tokens = ' '.join(tokens).lower()\n",
    "    doc_tokens = doc_tokens.encode('ascii', 'ignore')\n",
    "    input_file.write('%s %s\\n' % (doc_id, doc_tokens))\n",
    "print(id_, \"news group doc for training\")\n",
    "    \n",
    "id_ = 0\n",
    "for doc in news_docs_test:\n",
    "    doc_id = 'news_test_%i' % id_\n",
    "    id_ = id_ + 1\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    doc_tokens = ' '.join(tokens).lower()\n",
    "    doc_tokens = doc_tokens.encode('ascii', 'ignore')\n",
    "    input_file.write('%s %s\\n' % (doc_id, doc_tokens))\n",
    "print(id_, \"news group doc for test\")\n",
    "    \n",
    "id_ = 0\n",
    "for doc in reuters_docs_train:\n",
    "    doc_id = 'reuters_train_%i' % id_\n",
    "    id_ = id_ + 1\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    doc_tokens = ' '.join(tokens).lower()\n",
    "    doc_tokens = doc_tokens.encode('ascii', 'ignore')\n",
    "    input_file.write('%s %s\\n' % (doc_id, doc_tokens))\n",
    "print(id_, \"reuters doc for training\")\n",
    "    \n",
    "id_ = 0\n",
    "for doc in reuters_docs_test:\n",
    "    doc_id = 'reuters_test_%i' % id_\n",
    "    id_ = id_ + 1\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    doc_tokens = ' '.join(tokens).lower()\n",
    "    doc_tokens = doc_tokens.encode('ascii', 'ignore')\n",
    "    input_file.write('%s %s\\n' % (doc_id, doc_tokens))\n",
    "print(id_, \"reuters doc for test\")\n",
    "    \n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29634\n",
      "29634\n"
     ]
    }
   ],
   "source": [
    "docList = []\n",
    "\n",
    "for idx in range(len(news_docs_train)):\n",
    "    docList.append('news_train_' + str(idx))\n",
    "\n",
    "for idx in range(len(news_docs_test)):\n",
    "    docList.append('news_test_' + str(idx))\n",
    "\n",
    "for idx in range(len(reuters_docs_train)):\n",
    "    docList.append('reuters_train_' + str(idx))\n",
    "    \n",
    "for idx in range(len(reuters_docs_test)):\n",
    "    docList.append('reuters_test_' + str(idx))\n",
    "\n",
    "print(len(docList))\n",
    "docList = set(docList)\n",
    "print(len(docList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained documents Vectors\n",
      "29634 document vectors are loaded.\n",
      "11314 news training examples\n",
      "7532 news training examples\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "def load_pretrain_doc_voc():\n",
    "    print('Loading pretrained documents Vectors')\n",
    "    doc_vector = {}\n",
    "    f = open(os.path.join('Model', 'vectors300.txt'), \"r\") # vectors100.txt vectors300.txt\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in docList:\n",
    "            coefs = np.array([float(x) for x in values[1].split(',')])\n",
    "            doc_vector[word] = coefs\n",
    "    f.close()\n",
    "    return doc_vector\n",
    "\n",
    "doc_vector = load_pretrain_doc_voc()\n",
    "print('%s document vectors are loaded.' % len(doc_vector))\n",
    "\n",
    "news_x_train = []\n",
    "for idx in range(len(news_docs_train)):\n",
    "    news_x_train.append(doc_vector['news_train_' + str(idx)])\n",
    "print(len(news_x_train), 'news training examples')\n",
    "\n",
    "news_x_test = []\n",
    "for idx in range(len(news_docs_test)):\n",
    "    news_x_test.append(doc_vector['news_test_' + str(idx)])\n",
    "print(len(news_x_test), 'news training examples')\n",
    "\n",
    "print(len(news_x_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test pretrained document vector for document classification:\n",
      "RBF:Correct rate = 0.7006107275624005 When C = 8\n",
      "RBF:Correct rate = 0.7282262347318109 When C = 16\n",
      "RBF:Correct rate = 0.7338024429102497 When C = 32\n",
      "RBF:Correct rate = 0.7430961232076474 When C = 64\n",
      "RBF:Correct rate = 0.7458842272968667 When C = 128\n",
      "RBF:Correct rate = 0.74429102496017 When C = 256\n",
      "Linear:Correct rate = 0.7138874137015401 When C = 0.01\n",
      "Linear:Correct rate = 0.7400424853956453 When C = 0.05\n",
      "Linear:Correct rate = 0.7497344662772172 When C = 0.5\n",
      "Linear:Correct rate = 0.7482740308019118 When C = 1.0\n",
      "Linear:Correct rate = 0.7458842272968667 When C = 2.0\n",
      "Linear:Correct rate = 0.739644184811471 When C = 4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting test pretrained document vector for document classification:\")\n",
    "X = news_x_train\n",
    "y = news_y_train\n",
    "C1 = [8, 16, 32, 64, 128, 256]\n",
    "X_test = news_x_test\n",
    "y_test = news_y_test\n",
    "C2 = [0.01, 0.05, 0.5, 1.0, 2.0, 4.0]\n",
    "svm_test(X, y, C1, X_test, y_test, C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 4.2 Training Doc2Vec using gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['from', 'lerxst', 'wam', 'umd', 'edu', 'where', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst'], tags=['news_train_0']), TaggedDocument(words=['from', 'guykuo', 'carson', 'washington', 'edu', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'for', 'si', 'clock', 'reports', 'keywords', 'si', 'acceleration', 'clock', 'upgrade', 'article', 'shelley', 'qvfo', 'innc', 'organization', 'university', 'of', 'washington', 'lines', 'nntp', 'posting', 'host', 'carson', 'washington', 'edu', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'on', 'cards', 'and', 'adapters', 'heat', 'sinks', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', 'and', 'floppies', 'are', 'especially', 'requested', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', 'haven', 'answered', 'this', 'poll', 'thanks', 'guy', 'kuo', 'guykuo', 'washington', 'edu'], tags=['news_train_1'])]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Define a Function to Preprocess Text\n",
    "def create_training_data_set():\n",
    "    \n",
    "    train_corpus = []\n",
    "    for idx in range(len(news_docs_train)):\n",
    "        # For training data, add tags\n",
    "        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(news_docs_train[idx]),\n",
    "                                                                 ['news_train_' + str(idx)]))\n",
    "    \n",
    "    for idx in range(len(news_docs_test)):\n",
    "        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(news_docs_test[idx]),\n",
    "                                                                 ['news_test_' + str(idx)]))\n",
    "    \n",
    "    for idx in range(len(reuters_docs_train)):\n",
    "        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(reuters_docs_train[idx]),\n",
    "                                                                 ['reuters_train_' + str(idx)]))\n",
    "    \n",
    "    for idx in range(len(reuters_docs_test)):\n",
    "        train_corpus.append(gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(reuters_docs_test[idx]),\n",
    "                                                                 ['reuters_test_' + str(idx)]))\n",
    "    return train_corpus\n",
    "\n",
    "# load training files and test files\n",
    "train_corpus = create_training_data_set()\n",
    "\n",
    "# show first two training data\n",
    "print(train_corpus[0])\n",
    "print(news_docs_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 31min 58s\n"
     ]
    }
   ],
   "source": [
    "# Define the Doc2Vec model\n",
    "# with a vector size with 50 words and iterating over the training corpus 55 times\n",
    "# set the minimum word count to 2 in order to give higher frequency words more weighting\n",
    "# Model accuracy can be improved by increasing the number of iterations\n",
    "model = gensim.models.doc2vec.Doc2Vec(dm = 1, dm_concat = 1, size=300, window=10, min_count=2, negative=10, iter=100, workers = 4)\n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save(\"Model/gensim300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 news training examples\n",
      "7532 news training examples\n",
      "Starting test document vector generated by gensim for document classification:\n",
      "RBF:Correct rate = 0.3332448220924057 When C = 0.5\n",
      "RBF:Correct rate = 0.35661178969729157 When C = 1.0\n",
      "RBF:Correct rate = 0.37121614445034523 When C = 2.0\n",
      "RBF:Correct rate = 0.38528943175783326 When C = 4.0\n",
      "RBF:Correct rate = 0.4013542219861922 When C = 8.0\n",
      "RBF:Correct rate = 0.40693043016463093 When C = 16\n",
      "RBF:Correct rate = 0.41768454593733406 When C = 32\n",
      "RBF:Correct rate = 0.42644715878916617 When C = 64\n",
      "RBF:Correct rate = 0.435740839086564 When C = 128\n",
      "RBF:Correct rate = 0.4403876792352629 When C = 256\n",
      "Linear:Correct rate = 0.5130111524163569 When C = 0.5\n",
      "Linear:Correct rate = 0.5289431757833245 When C = 1.0\n",
      "Linear:Correct rate = 0.5426181625066384 When C = 2.0\n",
      "Linear:Correct rate = 0.5524429102496017 When C = 4.0\n",
      "Linear:Correct rate = 0.5584174190122145 When C = 8.0\n",
      "Linear:Correct rate = 0.5606744556558683 When C = 16\n",
      "Linear:Correct rate = 0.5539033457249071 When C = 32\n",
      "Linear:Correct rate = 0.5416887944768985 When C = 64\n",
      "Linear:Correct rate = 0.4843335103558152 When C = 128\n",
      "Linear:Correct rate = 0.4472915560276155 When C = 256\n"
     ]
    }
   ],
   "source": [
    "# Test document vector generated by gensim for document classification\n",
    "# model = Doc2Vec.load(fname)\n",
    "doc_vector = model.docvecs\n",
    "\n",
    "news_x_train = []\n",
    "for idx in range(len(news_docs_train)):\n",
    "    news_x_train.append(doc_vector['news_train_' + str(idx)])\n",
    "print(len(news_x_train), 'news training examples')\n",
    "\n",
    "news_x_test = []\n",
    "for idx in range(len(news_docs_test)):\n",
    "    news_x_test.append(doc_vector['news_test_' + str(idx)])\n",
    "print(len(news_x_test), 'news training examples')\n",
    "\n",
    "print(\"Starting test document vector generated by gensim for document classification:\")\n",
    "X = news_x_train\n",
    "y = news_y_train\n",
    "X_test = news_x_test\n",
    "y_test = news_y_test\n",
    "svm_test(X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
