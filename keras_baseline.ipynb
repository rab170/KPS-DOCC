{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate a Simple MLP on the Reuters newswire Topic Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "# different words will be kept (The length of the vocabulary)\n",
    "max_words = 2000\n",
    "# seed for split the reuters newswire dataset\n",
    "seed = 113\n",
    "# how many records from the dataset are used for test\n",
    "test_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8982 train sequences\n",
      "the wattie nondiscriminatory mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs\n",
      "2246 test sequences\n",
      "the in wants intermediate 3 how types could mln at against 2 guidelines vs end products opec he will will along results and willingly exports 3 purchased each it stubbornly profit 3 avondale profit agreement in around corp should for 3 cannot mln in ended said avondale a 54 but 3 stubbornly a only generally political primarily date other under well a in british rate gain if demand at an div its taking report montreal systems were is and production had vs 000 7 issued year for 0 a in buffer rate loss may results tariffs dlrs is and from 39 s 000 for 3 cannot pre 50 east that in foods products opec last 1 fall change 1 approval in 15 2 said in opec 28 but for 3 cannot cts systems which is half be 6 iffezheim uobm said president amount pct dlrs\n",
      "46 classes\n"
     ]
    }
   ],
   "source": [
    "# build inverse word dictionary\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "inverse_word_dict = np.ndarray(shape=(len(word_index)+1,), dtype=object)\n",
    "for key in word_index:\n",
    "    index = word_index[key]\n",
    "    inverse_word_dict[index] = key\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(test_split=0.2)\n",
    "\n",
    "print(len(x_train), 'train sequences are loaded.\\n')\n",
    "print(' '.join(inverse_word_dict[x_train[0]]), '\\n')\n",
    "print(len(x_test), 'test sequences are loaded.\\n')\n",
    "print(' '.join(inverse_word_dict[x_test[0]]), '\\n')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing sequence data...\n",
      "\t x_train_m shape: (8982, 2000)\n",
      "\t x_test_m shape: (2246, 2000) \n",
      "\n",
      "Convert class vector to binary class matrix\n",
      "\t(for use with categorical_crossentropy)\n",
      "\t y_train shape: (8982, 46)\n",
      "\t y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "# mode: one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\").\n",
    "x_train_m = tokenizer.sequences_to_matrix(x_train, mode = 'binary')\n",
    "x_test_m = tokenizer.sequences_to_matrix(x_test, mode = 'binary')\n",
    "print('\\t x_train_m shape:', x_train_m.shape)\n",
    "print('\\t x_test_m shape:', x_test_m.shape, '\\n')\n",
    "\n",
    "print('Convert class vector to binary class matrix:\\n\\t(for use with categorical_crossentropy)')\n",
    "y_train_m = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_m = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('\\t y_train shape:', y_train_m.shape)\n",
    "print('\\t y_test shape:', y_test_m.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/10\n",
      "10182/10182 [==============================] - 7s - loss: 15.2174 - acc: 0.0559 - val_loss: 15.1710 - val_acc: 0.0583\n",
      "Epoch 2/10\n",
      "10182/10182 [==============================] - 6s - loss: 15.2532 - acc: 0.0536 - val_loss: 15.2211 - val_acc: 0.0557\n",
      "Epoch 3/10\n",
      "10182/10182 [==============================] - 6s - loss: 15.2886 - acc: 0.0515 - val_loss: 15.2211 - val_acc: 0.0557\n",
      "Epoch 4/10\n",
      "10182/10182 [==============================] - 6s - loss: 15.2902 - acc: 0.0514 - val_loss: 15.2211 - val_acc: 0.0557\n",
      "Epoch 5/10\n",
      "10182/10182 [==============================] - 6s - loss: 15.2902 - acc: 0.0514 - val_loss: 15.2211 - val_acc: 0.0557\n",
      "Epoch 6/10\n",
      "10182/10182 [==============================] - 6s - loss: 15.2918 - acc: 0.0513 - val_loss: 15.2211 - val_acc: 0.0557\n",
      "Epoch 7/10\n",
      "10182/10182 [==============================] - 6s - loss: 15.2902 - acc: 0.0514 - val_loss: 15.2211 - val_acc: 0.0557\n",
      "Epoch 8/10\n",
      "10182/10182 [==============================] - 6s - loss: 15.2934 - acc: 0.0512 - val_loss: 15.2211 - val_acc: 0.0557\n",
      "Epoch 9/10\n",
      "10182/10182 [==============================] - 6s - loss: 15.2918 - acc: 0.0513 - val_loss: 15.2211 - val_acc: 0.0557\n",
      "Epoch 10/10\n",
      "10182/10182 [==============================] - 6s - loss: 15.2902 - acc: 0.0514 - val_loss: 15.2211 - val_acc: 0.0557\n",
      "7424/7532 [============================>.] - ETA: 0s\n",
      "Test score: 15.283515053\n",
      "Test accuracy: 0.0517790759466\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(x_train_m, y_train_m,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                    verbose = 1,\n",
    "                    validation_split = 0.1)\n",
    "\n",
    "score = model.evaluate(x_test_m, y_test_m,\n",
    "                       batch_size = batch_size, verbose = 1)\n",
    "\n",
    "print('\\nTest score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
